{
 "cells": [
  {
   "cell_type": "raw",
   "id": "230cf7a9-922c-4d6c-9d30-8ad0b4f0eb35",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"20 Questions with different LLM's\"\n",
    "description: \"Which LLM is the best at 20 questions?\"\n",
    "author: \"Kian Ambrose\"\n",
    "date: \"02/21/2025\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - Chat GPT 4o\n",
    "  - 20 Questions\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08966507-0a40-418d-b6e4-ba832e136e99",
   "metadata": {},
   "source": [
    "# Format\n",
    "\n",
    "I asked 4 different LLM's to play 20 questions with me. In this game I had the LLM guess the thing that I was thinking of. In each of the games, the thing I was \"thinking\" of was a notebook. Let's compare how they performed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ec5e7a-8ca9-4cca-904c-984dc3b61db8",
   "metadata": {},
   "source": [
    "# Chat GPT 4o\n",
    "\n",
    "<img src=\"Chat 20 Q.png\" width=\"50%\"/>\n",
    "\n",
    "<img src=\"Chat 20 Q2.png\" width=\"50%\"/>\n",
    "\n",
    "Chat GPT was not able to decipher what was said. The question that probably led to its downfall was the question \"Is it used for writing or drawing?\" After this question, Chat GPT moved from questions that narrow the choices down to direct guesses. While a notebook is used when writing and drawing, Chat GPT interpreted my response to mean that I was thinking about a writing or drawing instrument.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b7aba6-09b5-4151-b619-c6df7267aced",
   "metadata": {},
   "source": [
    "# Copilot\n",
    "<img src=\"Copilot.png\" width=\"50%\"/>\n",
    "<img src=\"Copilot 2.png\" width=\"50%\"/>\n",
    "\n",
    "Copilot was able to decipher what I was thinking of. Copilot started out with very similar questions to the ones asked by Chat GPT. Where Copilot separated itself (probably for the better) was when it continued asking general questions rather than trying to directly guess what I was thinking about. I think this is a strategy that a human would follow more closely. Another difference was that Copilot would offer up examples of things that it was asking about. For example, it said \" Is it something you use to organize papers, like a stapler or paperclip?\". Here, Copilot made it clear what it meant by \"something you use to organize papers.\" I think this would have helped Chat GPT in its process, especially with its question about whether it was something I write or draw with.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19383552-8675-4b5d-9c16-106ba311a2a4",
   "metadata": {},
   "source": [
    "# Gemini\n",
    "<img src=\"Gemini.png\" width=\"50%\"/>\n",
    "<img src=\"Gemini 2.png\" width=\"50%\"/>\n",
    "<img src=\"Gemini 3.png\" width=\"50%\"/>\n",
    "\n",
    "\n",
    "Gemini was not able to determine what I was thinking of. Gemini's first 2 question differed from Copilot and Chat GPT. Instead of asking if it was living or man made, it first asked if it was an animal and then if it was a plant. Finally on the third question it asked if it was manufactured. This process seems backwards. Instead of asking a specific question, like if it is an animal, it first should have asked if it was living. This would have saved an additional question and is more consistent with how a human would play the game. After the first few questions, Gemini was got on the same track as Copilot and began asking about if it was \n",
    "found in a house and where it was found in the house. Unfortunately, it was unable to ask questions that were specific to a notebook, so it seemed to get a little lost after \"Is it something related to reading?\". Before this point I thought it might narrow down the questioning, however, the following question was \"Is it something you use to tell time?\" which showed me that it had no idea what I was thinking of.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4cf5ca-a0c1-49e8-b2f5-f13b8a0594cf",
   "metadata": {},
   "source": [
    "# Llama\n",
    "<img src=\"Llama.png\" width=\"50%\"/>\n",
    "<img src=\"Llama 2.png\" width=\"50%\"/>\n",
    "\n",
    "Llama probably performed the worst at this game. It actually started out asking good questions, starting from the most general and then narrowing the questions down. It followed a similar process initially to Copilot. However, after question 13, it started repeating questions. This is not a good approach in 20 questions because you waste questions. No other LLM made the mistake of re asking the same questions. Not only did Llama ask the same question twice, it asked it using the same exact wording. It then proceeded to make the mistake a second time. In addition, Llama did not identify when the game was over. Instead of admitting defeat after question 20, it continued on to question 21. It seems that it was unable to follow directions, or there was some memory leak where it began forgetting what I had already told it.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c5785c-5279-493c-8d04-e7206837eccb",
   "metadata": {},
   "source": [
    "# Overall\n",
    "\n",
    "Overall, the LLM that performed the best at this game was Copilot. It was the only one to correctly identify what I was thinking of. It also followed a process that aligns with the general consensus of a \"smart\" way to play the game (starting with general questions before narrowing down). While there were many similarities among the different LLM's, Copilot was able to harness the best qualities of the group and correctly guess the object.\n",
    "\n",
    "Going forward it would be interesting to see if these LLM's are able to improve their question asking ability to improve at 20 questions. It would also be interesting to repeat this experiment multiple times to see if the results change."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
