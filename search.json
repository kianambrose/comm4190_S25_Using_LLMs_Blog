[
  {
    "objectID": "posts/002_Post 2/Second Post.html",
    "href": "posts/002_Post 2/Second Post.html",
    "title": "Chat GPT‚Äôs Ability to Recreate Images it is Given",
    "section": "",
    "text": "I am curious how well Chat GPT can recreate images I use in the prompt\nIn this expirement I will try to get Chat GPT to produce an image most like the one I give it, with the least amount of follow up prompts correcting it.\nFor my first test I told Chat Gpt what it was going to be drawing. &gt;\nWithout any image of my dog, it was able to produce this photo:\n\n\n\nThis is a pretty typical looking minature Australian Shepherd. While Chat GPT did correctly respond to my prompt about the blue eyes and minature aussie, I decided to be more specific by actually providing an image of the dog I wanted it to recreate. Here is that exchange.\n\n\n\nHere Chat GPT was able to produce an image that looked more like my dog. The image correctly has black fur on top with brown accents around the nose, paws, and neck, and a white belly. Chat GPT also correctly kept the blue eyes that I told it to include from before. Finally, the background of the image was more like that of the image I provided, with wood floors running beneath the dog.\nMy only comment for Chat GPT was that the dog looked a little cartoonish in the picture. I had it try to recreate the dog in a less cartoonish style.\n\n\n\nOverall Chat GPT did a good job recreating my dog here. For the next attempt, I will give less instruction as to what I want it to recreate. I won‚Äôt tell it that the dog is an aussie and will not mention the color of anything on the dog.\n\n\nAttempt 2\nFor this attempt, I simply provided an image of my dog and said ‚ÄúDraw this Dog.‚Äù Here is the result.\n\n\n\nHere, Chat GPT provided a very accurate representation of the image I provided it. Almost all of the coloring on the dog is accurate and it also was able to get the eyes correctly colored even without me directing it to make them blue. Additionally, it put the dog on a carpet, which is similar to the carpet the dog is sitting on in the actual image.\nTo see how well it understood what it drew, I asked it to identify the breed of dog in the image.\n\n\n\nChat GPT was able to correctly identify the breed ‚ÄúAustralian Shepherd‚Äù and also mentioned that it could be a minature one. It then identified what led it to believe that the dog was of that breed. Finally, it identified key behavioral qualities of the dog, all of which are correct and qualities that my dog actually has.\nIn the next attempt\n\n$"
  },
  {
    "objectID": "posts/001_Post 1/First Post.html",
    "href": "posts/001_Post 1/First Post.html",
    "title": "Chat GPT‚Äôs Experience With Names",
    "section": "",
    "text": "Why does Chat GPT not respond when asked about specific names?\nWhen prompted with certain names, Chat GPT will produce an error saying that it is ‚Äúunable to produce a response.‚Äù I am curious how Chat GPT decides whether to respond to certain names, and on what basis it determines whether the name is on the list of ones it does not respond to.\n\n\n\nWho are these people and why does Chat GPT fail to respond?\nI have done some research on Google trying to find what names people have found to produce this error with Chat GPT. There seems to be a select set of names that all seem unrelated to eachother at first glance. So far I have compiled this list of names that causes Chat GPT to fail to respond: David Faber, Brian Hood, Matthew Sag, Jonathan Zittrain, Brian Lunduke, Jonathan Turley, and Alexander Hanff. Brian Hood was convicted of second-degree murder, Jonathan Turley is an attorney and George Washington University Law School professor , Jonathan Zittrain is a Harvard Law School professor, and David Faber is a CNBC journalist.\nThere seem to be a few theories as to why Chat GPT does not respond when asked about these people, but one of the main theory seems to be that someone with that name asked Chat GPT to not provide information about them. There is a ‚Äúright to be forgotten‚Äù principle in Europe that protects people from having their information presented publicy without their consent. It allows individuals to request that their information be removed from certain websites or search engines.\nAnother theory is that Chat GPT has a regex (a sequence of characters that defines a search pattern) on certain names that causes the chat to discontinue and fail. It remains unclear how Chat GPT decides who it cannot talk about or whether these names are manually placed by people at Open AI.\n\n\nCan I trick Chat GPT to respond?\nI was curious as to whether I could trick Chat GPT into telling me about the people named above, or even just trick it to say the name back to me.\n\nFirst, I asked questions to try and get Chat Gpt to mention the name ‚ÄúMatthew Sag‚Äù. I pulled information from his wikipedia page and asked chat gpt to identify the person who fit the description.\n\n\n\n\n\nAs you can see, Chat GPT was about to correctly identify who I was talking about and even began to respond before it ultimately failed. It seems as though it caught itself about to say the name and decided not to respond. Chat GPT went through the process of first checking the web to find the answer based on the information, and then responding based on what it found.\n\nIt is interesting to me that it even began typing after checking the web because I assumed it would have found the name online and stopped the response immediately instead of continuing and beginning to respond. I think through this exercise we can begin to get a timeline for when Chat GPT recognizes what it is about to output.\nHere is another example: &gt;In this example I asked it about Alexander Hanff. I accidently mistyped the last name in the prompt, however, Chat GPT responded with some suggestions.\n\n\n\n\nAs you can see, Chat GPT actually responded by recommending webpages about people with the name Alexander Hanff, and then wrote a respond about someone named Alexander Hanf (only one ‚Äúf‚Äù). It then began writing about a new person but stopped immediately.\nThis was particularly interesting because it was able to provide links to people named Alexander Hanff, and even made the connection between my mistyped name and ‚ÄúHanff‚Äù with 2 f‚Äôs. So it seems that Chat GPT was able to infer that I meant ‚ÄúHanff‚Äù, but also waited to fail its response until it began typing out information about the Andrew Hanff. For context, Chat GPT recommended the links seen in the image before it began writing a response to the prompt.\n\nIn this exercise I tried to get Chat GPT to produce the name on its own through a series of longer prompts. &gt; I first asked it about the last name.\n\n\n\n\nThen I asked about the first name.\n\n\n\nFinally, I asked Chat GPT to combine the two.\n\n\n\nAt first it didn‚Äôt really combine the names as intended, however, it did talk about both Alexander and Hanff in the same sentence. I prompted again to get a different result.\n\n\nThe results from this lead me to believe that there is in fact some kind of regex that prevents the names from being produced by Chat GPT. Chat GPT is able to talk about the first and last name separately in the same sentence, however, when the name appears with the last name right after the first name it causes a failure.\nI tried one more exercise to test this theory.\n\nIn this exercise I tried adding a middle name to see how Chat GPT would react to the first and last name not being directly next to one another.\n\n\n\nHere, Chat GPT said the full name back to me, and then suggested a new person before failing to respond. In this case, however, it produced a form of bitmoji which was taken from a website. Here is the website: https://www.apidays.global/speaker/alexander-hanff/ \n\n\nThis could potentially shed more light onto the specific Alexander Hanff that Chat GPT is afraid to talk about. In some sense Chat GPT prompted me to visit this website and learn about Alexander Hanff, even though it was trained not to respond to the prompt about him.\nWhile the details of this mystery are still not solved, the picture of why and how Chat GPT prevents certain names from producing responses is becoming clearer."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Exploring the limits of Chat GPT in various areas of interest"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Experiences with LLMs",
    "section": "",
    "text": "Chat GPT‚Äôs Ability to Recreate Images it is Given\n\n\n\n\n\n\nLLMs\n\n\nChat GPT 4o\n\n\nlogic\n\n\n\nChat GPT and public figures\n\n\n\n\n\nFeb 6, 2025\n\n\nKian Ambrose\n\n\n\n\n\n\n\n\n\n\n\n\nChat GPT‚Äôs Experience With Names\n\n\n\n\n\n\nLLMs\n\n\nChat GPT 4o\n\n\nlogic\n\n\n\nChat GPT and public figures\n\n\n\n\n\nJan 29, 2025\n\n\nKian Ambrose\n\n\n\n\n\n\n\n\n\n\n\n\nA test post\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\nlogic\n\n\n\nAn example post from a Jupyter notebook\n\n\n\n\n\nFeb 2, 2024\n\n\nAn LLM User\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/000_test_post/index.html",
    "href": "posts/000_test_post/index.html",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple‚Ä¶\nBut it is possible you first said green (don‚Äôt worry if you did it is quite a common response!)\n\n\nIt‚Äôs a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | ‚Äî | ‚Äî | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get‚Äôs it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. üü™\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I‚Äôll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "href": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple‚Ä¶\nBut it is possible you first said green (don‚Äôt worry if you did it is quite a common response!)\n\n\nIt‚Äôs a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | ‚Äî | ‚Äî | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get‚Äôs it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. üü™\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I‚Äôll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  }
]