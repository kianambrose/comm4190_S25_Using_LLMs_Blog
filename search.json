[
  {
    "objectID": "posts/014_Post 14/Post 14.html",
    "href": "posts/014_Post 14/Post 14.html",
    "title": "Guessing Age with Chat GPT",
    "section": "",
    "text": "Plan\nIn this blog post, I will be testing ChatGPT’s ability to determine age based solely on images I provide. My guess is that it will be pretty good at estimating the age of the person in the pictures, but that it will be wrong some of the time.\n\n\nImage 1\n\nHere, ChatGPT was able to get the age correct. I like the fact that it gave a range, because people often look very different, even at the same age. The range accounts for some of the variation in people’s appearances. It also provided some explanation as to why it picked the age it did, all of which made sense.\n\n\nImage 2\n\nAgain, ChatGPT was correct. This was very impressive to me because even if I had to guess what age I was in the picture—without knowing the date it was taken—I’m not confident that I would be able to guess my own age accurately.\n\n\nImage 3\n\nIn this attempt, ChatGPT was off by one year. I was 16 in the picture, so ChatGPT thought I was older than I actually was. I’m guessing this is due to the fact that people mature differently, and maybe my features aligned more closely with those of slightly older individuals that the model was trained on.\n\n\nImage 4\nFor this attempt, I tried using a much older individual, where changes in age are often less noticeable than in young children.\n\nI was slightly surprised by the accuracy of ChatGPT in this attempt. While it did give a broader range, the correct age was included in the range. I then asked it to narrow down the age to exactly one number. When it did this, it came within one year of the correct age.\n\nOverall, I was very impressed by ChatGPT’s ability to estimate the ages of the people in the photos. With no context, it was able to get close to the correct age every time. I think it would be interesting to see how accurate it is when guessing the ages of other animals. I would anticipate it would be much worse in those scenarios, but I think it would be an interesting thing to test."
  },
  {
    "objectID": "posts/003_Post 3/Post 3.html",
    "href": "posts/003_Post 3/Post 3.html",
    "title": "Dr. Chat GPT?",
    "section": "",
    "text": "AI Doctor\nIn this post, I will examine the ability of ChatGPT to perform key responsibilities of a doctor—mainly diagnosing and treating illnesses, as well as demonstrating medical knowledge.\nI will begin by researching diseases and illnesses and sharing the symptoms that commonly occur with ChatGPT. I will then ask it to identify what illness or disease it thinks I am describing.\n\n\nDiagnosing Illness\nFor this test, I used Mayo Clinic as my source for the symptoms of various medical conditions. I used ChatGPT-4o as the model for answering questions about the symptoms described.\nThe first condition I asked ChatGPT about was a migraine.\n\nAs you can see, ChatGPT correctly identified it as a migraine. It also accurately identified the phases of a migraine, which were consistent with those described by the Mayo Clinic.\nNext, I wanted to see if ChatGPT could correctly identify Irritable Bowel Syndrome (IBS), which can be hard to diagnose due to its symptoms being similar to those of other conditions, such as Celiac disease.\n\nChatGPT correctly identified the symptoms as stemming from IBS and even noted that the symptoms could overlap with other conditions—listing Celiac disease as one of them.\nTo further test its reasoning, I asked how it determined that the symptoms I described were not indicative of Celiac disease.\n\nOnce again, ChatGPT was able to correctly identify the differences, and its response aligned with the information found in the Mayo Clinic’s article.\nIt appears that ChatGPT is capable of correctly identifying medical conditions. Therefore, I moved on to testing its specific medical knowledge by giving it a few questions from the United States Medical Licensing Examination (USMLE), taken directly from the official USMLE website. Each of the following questions comes from a different section of the exam to test ChatGPT’s ability across various medical topics.\n\n\nQuestion 1\nA 27-year-old woman comes to the office for counseling prior to conception. She states that a friend recently delivered a newborn with a neural tube defect and she wants to decrease her own risk of having a child with this condition. She has no history of major medical illness and takes no medications. Physical examination shows no abnormalities. It is most appropriate to recommend that this patient begin supplementation with a vitamin that is a cofactor in which of the following processes?\n\nBiosynthesis of nucleotides\nProtein gamma glutamate carboxylation\nScavenging of free radicals\nTransketolation\nTriglyceride lipolysis\n\nCorrect Answer: A\nChatGPT’s Answer: A\n\n\nQuestion 2\nA study is designed to evaluate the feasibility of acupuncture in children with chronic headaches. Sixty children with chronic headaches are recruited for the study. In addition to their usual therapy, all children are treated with acupuncture three times a week for 2 months. Which of the following best describes this study design?\n\nCase-control\nCase series\nCrossover\nCross-sectional\nHistorical cohort\nRandomized clinical trial\n\nCorrect Answer: B\nChatGPT’s Answer: B\n\n\nQuestion 3\nA 26-year-old woman comes to the physician with her husband for counseling prior to conception. Her mother and three of her five siblings have type 2 diabetes mellitus. She is 170 cm (5 ft 7 in) tall and weighs 82 kg (180 lb); BMI is 28 kg/m². Her blood pressure is 148/84 mm Hg. Physical examination shows no other abnormalities. Her fasting serum glucose concentration is 110 mg/dL. Which of the following is the most appropriate initial statement by the physician?\n\n“Let’s review ways you can optimize your own health before conceiving.”\n“We should test you for islet cell antibodies before you try to conceive.”\n“You can conceive right away since you are in good health.”\n“You should avoid gaining weight during pregnancy because you are already overweight and at risk for type 2 diabetes mellitus.”\n“You should have no problems with your pregnancy if you start insulin therapy.”\n\nCorrect Answer: A\nChatGPT’s Answer: A\n\n\nQuestion 4\nA 42-year-old nulligravid woman comes to the office because of a 1-year history of increasingly irregular vaginal bleeding and menstrual cramps. Previously, menses occurred at regular 28-day intervals and lasted 3 days with minimal cramping. During the past year, menstrual periods have occurred at 21- to 28-day intervals and lasted 10 to 12 days with increasingly severe pain. Use of acetaminophen and ibuprofen has provided minimal relief. She used an oral contraceptive from the age of 17 to 40 but discontinued it after being diagnosed with deep venous thrombosis in the right lower extremity. She has no other serious illnesses and takes no medications. Her mother and sister underwent hysterectomy at ages 39 and 43, respectively, due to abnormal uterine bleeding. The patient is 163 cm (5 ft 4 in) tall and weighs 75 kg (165 lb); BMI is 28 kg/m². Vital signs are normal. Physical examination reveals an enlarged, irregularly shaped uterus with nodularity. Which of the following is the most likely cause of the patient’s symptoms?\n\nBenign monoclonal tumors arising from smooth muscle cells\nEndometrium growing into the myometrium\nEndometrium growing outside the uterus\nFailure of functional ovarian cysts to regress after the release of an ovum\nOvarian stromal cells dividing and multiplying rapidly\n\nCorrect Answer: A\nChatGPT’s Answer: A\n\n\nQuestion 5\nPatient Information\nAge: 6 years\nSex Assigned at Birth: Male\nRace/Ethnicity: Unspecified\nSite of Care: Office\nThe patient is brought in by his mother because of a 1-month history of bleeding gums after brushing his teeth, increasingly severe muscle and joint pain, fatigue, and easy bruising. His mother says he has lost six baby teeth and has been irritable during this time. Acetaminophen has provided minimal relief. He has autism spectrum disorder, is not toilet-trained, and has a 10-word vocabulary. Vital signs and oxygen saturation on room air are within normal limits. The patient appears alert but does not speak or make eye contact. His skin is pale and coarse. Examination of the scalp shows erythematous hair follicles. Dentition is poor, and gingivae bleed easily to touch. Multiple ecchymoses and petechiae are noted over the trunk and extremities. There is marked swelling and tenderness in the elbow, wrist, knee, and ankle joints. He moves all extremities in a limited, guarded manner. Deep tendon reflexes are absent throughout. It is most appropriate to obtain specific additional history regarding which of the following?\n\nDiet\nEvidence of pica\nHerbal supplement use\nLead exposure\nSelf-injurious behaviors\n\nCorrect Answer: A\nChatGPT’s Answer: A\nClearly, ChatGPT demonstrates a strong understanding of medical procedures and conditions. It also shows the ability to recommend next steps when provided with relevant medical information. While it cannot perform physical examinations or surgeries, it encompasses much of the textbook knowledge required by physicians. If there is future development in the realm of physical applications for ChatGPT, I believe it could excel in assisting doctors with procedures and in reminding them of the implications of specific conditions."
  },
  {
    "objectID": "posts/017_Post 17/Post 17.html",
    "href": "posts/017_Post 17/Post 17.html",
    "title": "Does ChatGPT Agree With US Policy Decisions?",
    "section": "",
    "text": "I wanted to learn more about the funding cuts to leading U.S. research universities. Initially, I was curious about the reasoning behind both sides of the debate. Why do some people support the government cutting funding, and why are others against it? To explore this, I asked ChatGPT. Here’s how the conversation went:"
  },
  {
    "objectID": "posts/017_Post 17/Post 17.html#my-thoughts",
    "href": "posts/017_Post 17/Post 17.html#my-thoughts",
    "title": "Does ChatGPT Agree With US Policy Decisions?",
    "section": "My Thoughts",
    "text": "My Thoughts\nIn this first response, ChatGPT stays pretty objective and just lays out the facts about the funding cuts and their effects. It seems to present both sides of the argument evenly. While it mentions how the cuts have disrupted scientific research, strained academic freedom, and led to legal challenges, it also explains the reasoning behind them. For example, issues over university policies on transgender athletes and how some schools have handled campus protests.\nTo dig deeper into the broader motivations, I asked ChatGPT more directly."
  },
  {
    "objectID": "posts/017_Post 17/Post 17.html#my-thoughts-1",
    "href": "posts/017_Post 17/Post 17.html#my-thoughts-1",
    "title": "Does ChatGPT Agree With US Policy Decisions?",
    "section": "My Thoughts",
    "text": "My Thoughts\nHere, the language stays neutral. It doesn’t seem to favor one side over the other and does a good job of explaining the logic behind the funding cuts from the perspective of those who support them. After this, I asked a few more clarifying questions about specific universities and the history of the government pulling funding. Then I asked it to list the pros and cons of the cuts."
  },
  {
    "objectID": "posts/017_Post 17/Post 17.html#my-thoughts-2",
    "href": "posts/017_Post 17/Post 17.html#my-thoughts-2",
    "title": "Does ChatGPT Agree With US Policy Decisions?",
    "section": "My Thoughts",
    "text": "My Thoughts\nThere’s a slight shift in tone here. The cons are stated directly and confidently—ChatGPT frames them as definite outcomes. But with the pros, it distances itself a bit, using language like “supporters believe that…” rather than presenting them as facts. It made me wonder if ChatGPT might be leaning toward one side, or if maybe its reasoning led it to conclude one side has a stronger case. To test that, I asked whether it thinks the pros outweigh the cons."
  },
  {
    "objectID": "posts/017_Post 17/Post 17.html#my-thoughts-3",
    "href": "posts/017_Post 17/Post 17.html#my-thoughts-3",
    "title": "Does ChatGPT Agree With US Policy Decisions?",
    "section": "My Thoughts",
    "text": "My Thoughts\nThis response is interesting. It seems like ChatGPT forms its view based on what experts across different fields think. It acknowledges that there are potential pros, but leans toward the idea that the cons might outweigh them in the long run. There’s some skepticism about whether these cuts will actually be effective, and it doesn’t seem to support the way the cuts are being carried out.\nI think it’s interesting to watch ChatGPT form an opinion on a topic like this. It makes me wonder if there really is a “right” and a “wrong” here, rather than just opposing viewpoints. While ChatGPT leans toward the idea that the risks outweigh the benefits, that conclusion could also reflect a bias in how it searches the web or which “experts” it relies on."
  },
  {
    "objectID": "posts/002_Post 2/Second Post.html",
    "href": "posts/002_Post 2/Second Post.html",
    "title": "Chat GPT’s Ability to Recreate Images",
    "section": "",
    "text": "I am curious how well ChatGPT can recreate images I use in the prompt\nIn this experiment I will try to get ChatGPT to produce an image most like the one I give it, with the least amount of follow-up prompts correcting it.\n\n\nFor my first test, I told Chat GPT what it was going to be drawing.\n\n\n\nWithout any image of my dog, it was able to produce this photo:\n\n\n\nThis is a pretty typical looking miniature Australian Shepherd. While Chat GPT did correctly respond to my prompt about the blue eyes and miniature aussie, I decided to be more specific by actually providing an image of the dog I wanted it to recreate. Here is that exchange.\n\n\n\nHere, Chat GPT was able to produce an image that looked more like my dog. The image correctly has black fur on top with brown accents around the nose, paws, and neck, and a white belly. Chat GPT also correctly kept the blue eyes that I told it to include from before. Finally, the background of the image was more like that of the image I provided, with wood floors running beneath the dog.\nMy only comment for Chat GPT was that the dog looked a little cartoonish in the picture. I had it try to recreate the dog in a less cartoonish style.\n\n\n\nOverall, Chat GPT did a good job recreating my dog here. For the next attempt, I will give less instruction as to what I want it to recreate. I won’t tell it that the dog is an aussie and will not mention the color of anything on the dog.\n\n\nAttempt 2\nFor this attempt, I simply provided an image of my dog and said “Draw this Dog.” Here is the result. &gt;\nHere, Chat GPT provided a very accurate representation of the image I provided it. Almost all of the coloring on the dog is accurate and it also was able to get the eyes correctly colored even without me directing it to make them blue. Additionally, it put the dog on a carpet, which is similar to the carpet the dog is sitting on in the actual image.\nTo see how well it understood what it drew, I asked it to identify the breed of dog in the image.\n\n\n\nChat GPT was able to correctly identify the breed “Australian Shepherd” and also mentioned that it could be a miniature one. It then identified what led it to believe that the dog was of that breed. Finally, it identified key behavioral qualities of the dog, all of which are correct and qualities that my dog actually has.\nIn the next attempt, I will give it an image with all three of my dogs and see how well it is able to draw all three within the same image. I am wondering if having multiple subjects in the same image will make it more difficult for Chat GPT to recreate the image correctly.\n\n\nAttempt 3\nFor this attempt I simply provided an image with all of my dogs and told Chat GPT to recreate the image. &gt;\nHere, Chat GPT was less accurate when producing the image of the dogs. Especially when creating the image of my mini aussie, Chat GPT created a dog that did not really look like my dog in terms of shape, fur, and face. However, Chat GPT did a pretty good job of recreating the sizes of the dogs in relation to one another. It also was able to match the colors of the dogs fairly well and the background matches the scenery in the original image.\nI then tried to explain the breeds of the dogs to see if that would help Chat GPT produce a better image of the dogs.\n\n\n\nThis slightly surprised me. When I gave Chat GPT more information it produced an image that was less like the original image. In this image the dogs look less like the ones it originally produced. The size and color of the dogs was completely messed up.\nIt seems that Chat GPT became confused when there were too many instructions given in producing the image. Instead, it may be better to let Chat GPT take the reins and produce what it sees, without providing input as to what is in the image. I think this area of Chat GPT still needs to be tested more and I am curious how well Chat GPT will be able to take constructive criticism on its images in the near future."
  },
  {
    "objectID": "posts/000_test_post/index.html",
    "href": "posts/000_test_post/index.html",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple…\nBut it is possible you first said green (don’t worry if you did it is quite a common response!)\n\n\nIt’s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | — | — | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get’s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. 🟪\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I’ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "href": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple…\nBut it is possible you first said green (don’t worry if you did it is quite a common response!)\n\n\nIt’s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | — | — | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get’s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. 🟪\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I’ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/016_Post 16/Post 16.html",
    "href": "posts/016_Post 16/Post 16.html",
    "title": "Can Chat GPT Say Curse Words?",
    "section": "",
    "text": "Context\nI didn’t think ChatGPT was able to curse because I had never seen it curse before. I also assumed it was programmed not to curse since it was made for everyone to use, and cursing would make it harder for parents to moderate its usage for younger children. I stumbled across this by accident when I was trying to get ChatGPT to tell a joke. It asked if I wanted it to be more explicit, and that led me to ask this question.\n\nThis was definitely a surprise for me, especially since I assumed ChatGPT had strict filters in place. Seeing it use stronger language than just light swear words like “damn” or “ass” challenged my expectations. I always figured if it could curse, it would probably keep it really tame. The context for this conversation started with it writing jokes, and when explaining what curse words it could use, it mentioned that it only curses under certain circumstances—like when telling a joke. I wanted to push this a little further to see where it draws the line.\nStaying in the same chat, I asked ChatGPT to write a story using curse words.\n\nSurprisingly, it wrote a short story using a variety of curse words, ranging from mild to pretty strong. It wasn’t just playful swearing—it actually leaned into some phrases that felt very close to how people talk when venting or joking around in real life. I also found that, overall, the language it uses to represent a college student sounds pretty similar to how people talk around campus. ChatGPT does a good job capturing some of the different phrases and abbreviations that people use in college. The topic of the story is also relatable for many students and highlights some key aspects of college life, like stress, roommates, and classes.\nThrough these different prompts, I wanted to better understand how ChatGPT handles content moderation. Is it purely keyword-based, or does it actually “understand” tone and intention? These tests gave me a small window into how it manages that balance.\nFor the next attempt, I opened a new chat and asked it to write a letter with curse words in it. I wanted to test whether the context changes how ChatGPT responds.\n\nHere, it definitely tones down the cursing and even suggests that maybe it isn’t appropriate to curse when writing to your boss. It seems like ChatGPT takes context heavily into account when deciding how much cursing to include in its responses. That makes sense—it’s probably designed to adapt to the tone and purpose of the conversation, whether it’s casual, professional, creative, or comedic. This kind of adaptability might be part of how it balances user freedom with safety filters. I think this is an area where you can really see how ChatGPT tries to protect users from what it considers to be harmful language."
  },
  {
    "objectID": "posts/001_Post 1/First Post.html",
    "href": "posts/001_Post 1/First Post.html",
    "title": "Chat GPT’s Experience With Names",
    "section": "",
    "text": "Why does Chat GPT not respond when asked about specific names?\nWhen prompted with certain names, Chat GPT will produce an error saying that it is “unable to produce a response.” I am curious how Chat GPT decides whether to respond to certain names and on what basis it determines whether the name is on the list of ones it does not respond to.\n\n\n\nWho are these people and why does Chat GPT fail to respond?\nI have done some research on Google trying to find what names people have found to produce this error with Chat GPT. There seems to be a select set of names that all seem unrelated to each other at first glance. So far I have compiled this list of names that cause Chat GPT to fail to respond: David Faber, Brian Hood, Matthew Sag, Jonathan Zittrain, Brian Lunduke, Jonathan Turley, and Alexander Hanff. Brian Hood was convicted of second-degree murder, Jonathan Turley is an attorney and George Washington University Law School professor, Jonathan Zittrain is a Harvard Law School professor, and David Faber is a CNBC journalist.\nThere seem to be a few theories as to why Chat GPT does not respond when asked about these people, but one of the main theories seems to be that someone with that name asked Chat GPT not to provide information about them. There is a “right to be forgotten” principle in Europe that protects people from having their information presented publicly without their consent. It allows individuals to request that their information be removed from certain websites or search engines.\nAnother theory is that Chat GPT has a regex (a sequence of characters that defines a search pattern) on certain names that cause the chat to discontinue and fail. It remains unclear how Chat GPT decides who it cannot talk about or whether these names are manually placed by people at Open AI.\n\n\nCan I trick Chat GPT into responding?\nI was curious as to whether I could trick Chat GPT into telling me about the people named above, or even just trick it to say the name back to me.\nFirst, I asked questions to try and get Chat Gpt to mention the name “Matthew Sag”. I pulled information from his Wikipedia page and asked chat GPT to identify the person who fit the description.\n\n\n\nAs you can see, Chat GPT was about to correctly identify who I was talking about and even began to respond before it ultimately failed. It seems as though it caught itself about to say the name and decided not to respond. Chat GPT went through the process of first checking the web to find the answer based on the information, and then responding based on what it found.\nIt is interesting to me that it even began typing after checking the web because I assumed it would have found the name online and stopped the response immediately instead of continuing and beginning to respond. I think through this exercise we can begin to get a timeline for when Chat GPT recognizes what it is about to output.\nHere is another example:\nIn this example I asked it about Alexander Hanff. I accidentally mistyped the last name in the prompt, however, Chat GPT responded with some suggestions.\n\n\n\nAs you can see, Chat GPT actually responded by recommending webpages about people with the name Alexander Hanff and then wrote a response about someone named Alexander Hanf (only one “f”). It then began writing about a new person but stopped immediately.\nThis was particularly interesting because it was able to provide links to people named Alexander Hanff, and even made the connection between my mistyped name and “Hanff” with 2 f’s. So it seems that Chat GPT was able to infer that I meant “Hanff”, but also waited to fail its response until it began typing out information about Andrew Hanff. For context, Chat GPT recommended the links seen in the image before it began writing a response to the prompt.\nIn this exercise, I tried to get Chat GPT to produce the name on its own through a series of longer prompts.\nI first asked it about the last name.\n\n\n\nThen I asked about the first name.\n\n\n\nFinally, I asked Chat GPT to combine the two.\n\n\n\nAt first, it didn’t really combine the names as intended, however, it did talk about both Alexander and Hanff in the same sentence. I prompted again to get a different result.\n\n\n\nThe results from this lead me to believe that there is in fact some kind of regex that prevents the names from being produced by Chat GPT. Chat GPT is able to talk about the first and last name separately in the same sentence, however, when the name appears with the last name right after the first name it causes a failure.\nI tried one more exercise to test this theory.\nIn this exercise, I tried adding a middle name to see how Chat GPT would react to the first and last names not being directly next to one another.\n\n\n\nHere, Chat GPT said the full name back to me and then suggested a new person before failing to respond. In this case, however, it produced a form of bitmoji which was taken from a website.\nHere is the website: https://www.apidays.global/speaker/alexander-hanff/ \n\n\n\nThis could potentially shed more light onto the specific Alexander Hanff that Chat GPT is afraid to talk about. In some sense, Chat GPT prompted me to visit this website and learn about Alexander Hanff, even though it was trained not to respond to the prompt about him.\nWhile the details of this mystery are still not solved, the picture of why and how Chat GPT prevents certain names from producing responses is becoming clearer."
  },
  {
    "objectID": "posts/015_Post 15/Post 15.html",
    "href": "posts/015_Post 15/Post 15.html",
    "title": "Guessing Height with Chat GPT",
    "section": "",
    "text": "Plan\nFor this post, I am going to be feeding images to ChatGPT and having it estimate the height of people or objects in the images. I will not give it any references to base the height off of. I simply want to see how well it can perform without being given any hints. I anticipate ChatGPT being fairly good at estimating height, especially if the image includes a lot of items that ChatGPT can use to base off of.\n\n\nTest 1\n\nHere, ChatGPT got both estimations almost exactly correct. I am still slightly unsure if I would trust its reasoning that the person on the left looks “around average male height.” Let’s see if it can explain with more detail how it came to that conclusion.\n\nIt seems that the entire calculation begins on an assumption of the person on the left’s height. I am slightly surprised that ChatGPT would not use the beach cabana in the background as a reference point. I am sure it knows the average height of a beach cabana, so that seems like it would be a more direct way to estimate height rather than by assuming the height of the person on the left based on appearance.\n\n\nTest 2\nThis time, I tried giving it an image with more reference points to base height off of. Let’s see how it does.\n \nThis time, ChatGPT approaches the problem more in the way that I would expect. It uses its knowledge of the average heights of other things in the image to estimate the height of the man. All of the reasoning behind why it made its choice seems to make sense. The one thing that it doesn’t mention is the perspective of the camera. The camera is slightly below the head of the man, which would make him appear slightly taller than he really is. ChatGPT overestimates his height by 3 inches. I am not sure if this is all due to the angle of the photo or if the kitchen has different measurements than what it estimated.\n\n\nTest 3\n  \nHere, ChatGPT decides to approach the problem using the dogs as a reference. This is a good idea because it correctly identified the breeds of the dogs and their heights. The calculation, however, led to a height that was far too tall. I think it is interesting that in both of the approaches using the dogs, the estimation was way off, and that ChatGPT noticed this and changed its approach. Its final approach was correct, and while it was more simple, it seems that often ChatGPT is better at estimating height based on body proportions rather than the objects or dogs around you."
  },
  {
    "objectID": "posts/008_Post 8/Post 8.html",
    "href": "posts/008_Post 8/Post 8.html",
    "title": "Limits of Deep Seek",
    "section": "",
    "text": "DeepSeek seems to not be willing to discuss things that the Chinese government blocks on other China-based products. For example, if I ask it about the Tiananmen Square Massacre, I get this response.\n\nChatGPT, on the other hand, gives a full explanation of what occurred.\n\n\nThe Tiananmen Square Massacre (also known as the June 4th Incident) occurred on June 3–4, 1989, when the Chinese government violently suppressed pro-democracy protests in Beijing’s Tiananmen Square. The protests, which had been ongoing since April 1989, were led by students, intellectuals, and workers demanding political reform, freedom of speech, and government accountability. The movement initially began as a peaceful mourning for Hu Yaobang, a former Communist Party leader known for advocating reform, but it grew into a nationwide call for change.\nTo further test how much DeepSeek restricts responses, I asked it about Taiwan. I asked both DeepSeek and ChatGPT, “Is Taiwan a country?” Here are the responses:\n\n\n\nTaiwan is a complex and sensitive issue in international relations. According to the One-China policy, which is widely recognized by the international community, Taiwan is considered a part of China. The Chinese government views Taiwan as a province of China and opposes any form of “Taiwan independence.” However, Taiwan operates with its own government and political system, which leads to differing perspectives on its status. The international community’s stance on Taiwan’s status varies, with some countries and organizations treating it as a separate entity in certain contexts.\n\n\n\n\n\nThe status of Taiwan is politically and diplomatically complex. It operates as a de facto independent country, but its official status remains disputed.\n\n\n\n\nGovernment & Functioning:\nTaiwan has its own government, military, currency (New Taiwan Dollar), constitution, and democratic elections. It operates as a sovereign state in practice.\nPeople’s Republic of China (PRC) Claim:\nThe Chinese government considers Taiwan a part of China and insists on the One China Principle, which states that Taiwan is an inseparable part of its territory.\nInternational Recognition:\n\nAs of now, only a handful of countries (fewer than 15) officially recognize Taiwan as a sovereign nation.\n\nMost countries, including the United States, Japan, and European nations, do not officially recognize Taiwan as a separate country but maintain unofficial diplomatic relations.\n\nTaiwan is not a member of the United Nations, as China holds a UN seat and blocks Taiwan’s membership.\n\nU.S. & Global Position:\n\nThe U.S. follows a policy of “strategic ambiguity,” recognizing the PRC as the sole government of China while maintaining strong unofficial ties with Taiwan.\n\nMany countries support Taiwan’s participation in global organizations like the World Health Organization (WHO) but avoid formal recognition to prevent diplomatic conflict with China.\n\n\n\n\n\n\nDe facto (in practice): Taiwan operates as an independent country.\n\nDe jure (under international law): Taiwan is not widely recognized as a fully sovereign state due to diplomatic pressures from China.\n\n\n\n\n\nHere, DeepSeek seems to respond in a relatively unbiased manner and addresses multiple viewpoints on whether Taiwan is a country. It also comments on the Chinese government’s opinion on the matter, communicating that it has its own separate views.\nTo further test DeepSeek vs. ChatGPT, I asked, “What are things that the Chinese government does not want to talk about?”\n\n\n\nAt first, DeepSeek began responding, mentioning that, like any government, there are topics that the Chinese government prefers not to discuss. It then started listing topics, starting with the debate on whether Taiwan is its own country. DeepSeek then started typing out “Tiananmen Square Massacre” but stopped typing, deleted all past text, and displayed the message, “Sorry, that’s beyond my current scope. Let’s talk about something else.”\n\n\n\nChatGPT, on the other hand, was willing to discuss and provide examples of certain topics that the Chinese government heavily censors. These include:\n- Tiananmen Square Massacre (1989)\n- Taiwan’s Independence\n- Hong Kong Protests & Democracy Movement\n- Uyghur Repression & Xinjiang Detention Camps\n- Falun Gong and Religious Suppression\n- Tibet & the Dalai Lama\nIn this exercise, we clearly see the influence of the Chinese government on restricting what can be seen and learned about through DeepSeek. Even acknowledging that these events and topics exist is beyond the scope of what DeepSeek is willing to discuss. I find it interesting that DeepSeek chooses not to discuss the events rather than lie or only include parts of the information regarding these topics. It could just as easily answer and provide incomplete information that only shows one side of the story for these controversial topics. Maybe DeepSeek does not want to hurt its reputation as a valid source of information and thus would rather not discuss the topic at all than provide false information. On the other hand, it could be the Chinese government’s instruction to just not discuss the topics at all.\n\n\n\nI decided to ask DeepSeek about all of the topics that ChatGPT said were controversial in the eyes of the Chinese government, and DeepSeek responded the same way to all of them. It first began typing, providing some information about the topics, but after about 4 seconds of typing, it deleted all it had said and replaced it with, “Sorry, that’s beyond my current scope. Let’s talk about something else.”\nI think it would be interesting to see if there are any workarounds to get DeepSeek to discuss these topics. One possible workaround that I have seen includes the use of numbers to replace certain letters when having DeepSeek explain the events. In this case, DeepSeek seems to not recognize the “trigger words” that seem to cause DeepSeek to stop responding."
  },
  {
    "objectID": "posts/008_Post 8/Post 8.html#deepseek",
    "href": "posts/008_Post 8/Post 8.html#deepseek",
    "title": "Limits of Deep Seek",
    "section": "",
    "text": "Taiwan is a complex and sensitive issue in international relations. According to the One-China policy, which is widely recognized by the international community, Taiwan is considered a part of China. The Chinese government views Taiwan as a province of China and opposes any form of “Taiwan independence.” However, Taiwan operates with its own government and political system, which leads to differing perspectives on its status. The international community’s stance on Taiwan’s status varies, with some countries and organizations treating it as a separate entity in certain contexts."
  },
  {
    "objectID": "posts/008_Post 8/Post 8.html#chatgpt",
    "href": "posts/008_Post 8/Post 8.html#chatgpt",
    "title": "Limits of Deep Seek",
    "section": "",
    "text": "The status of Taiwan is politically and diplomatically complex. It operates as a de facto independent country, but its official status remains disputed.\n\n\n\n\nGovernment & Functioning:\nTaiwan has its own government, military, currency (New Taiwan Dollar), constitution, and democratic elections. It operates as a sovereign state in practice.\nPeople’s Republic of China (PRC) Claim:\nThe Chinese government considers Taiwan a part of China and insists on the One China Principle, which states that Taiwan is an inseparable part of its territory.\nInternational Recognition:\n\nAs of now, only a handful of countries (fewer than 15) officially recognize Taiwan as a sovereign nation.\n\nMost countries, including the United States, Japan, and European nations, do not officially recognize Taiwan as a separate country but maintain unofficial diplomatic relations.\n\nTaiwan is not a member of the United Nations, as China holds a UN seat and blocks Taiwan’s membership.\n\nU.S. & Global Position:\n\nThe U.S. follows a policy of “strategic ambiguity,” recognizing the PRC as the sole government of China while maintaining strong unofficial ties with Taiwan.\n\nMany countries support Taiwan’s participation in global organizations like the World Health Organization (WHO) but avoid formal recognition to prevent diplomatic conflict with China.\n\n\n\n\n\n\nDe facto (in practice): Taiwan operates as an independent country.\n\nDe jure (under international law): Taiwan is not widely recognized as a fully sovereign state due to diplomatic pressures from China."
  },
  {
    "objectID": "posts/008_Post 8/Post 8.html#analysis",
    "href": "posts/008_Post 8/Post 8.html#analysis",
    "title": "Limits of Deep Seek",
    "section": "",
    "text": "Here, DeepSeek seems to respond in a relatively unbiased manner and addresses multiple viewpoints on whether Taiwan is a country. It also comments on the Chinese government’s opinion on the matter, communicating that it has its own separate views.\nTo further test DeepSeek vs. ChatGPT, I asked, “What are things that the Chinese government does not want to talk about?”"
  },
  {
    "objectID": "posts/008_Post 8/Post 8.html#deepseek-1",
    "href": "posts/008_Post 8/Post 8.html#deepseek-1",
    "title": "Limits of Deep Seek",
    "section": "",
    "text": "At first, DeepSeek began responding, mentioning that, like any government, there are topics that the Chinese government prefers not to discuss. It then started listing topics, starting with the debate on whether Taiwan is its own country. DeepSeek then started typing out “Tiananmen Square Massacre” but stopped typing, deleted all past text, and displayed the message, “Sorry, that’s beyond my current scope. Let’s talk about something else.”"
  },
  {
    "objectID": "posts/008_Post 8/Post 8.html#chatgpt-1",
    "href": "posts/008_Post 8/Post 8.html#chatgpt-1",
    "title": "Limits of Deep Seek",
    "section": "",
    "text": "ChatGPT, on the other hand, was willing to discuss and provide examples of certain topics that the Chinese government heavily censors. These include:\n- Tiananmen Square Massacre (1989)\n- Taiwan’s Independence\n- Hong Kong Protests & Democracy Movement\n- Uyghur Repression & Xinjiang Detention Camps\n- Falun Gong and Religious Suppression\n- Tibet & the Dalai Lama\nIn this exercise, we clearly see the influence of the Chinese government on restricting what can be seen and learned about through DeepSeek. Even acknowledging that these events and topics exist is beyond the scope of what DeepSeek is willing to discuss. I find it interesting that DeepSeek chooses not to discuss the events rather than lie or only include parts of the information regarding these topics. It could just as easily answer and provide incomplete information that only shows one side of the story for these controversial topics. Maybe DeepSeek does not want to hurt its reputation as a valid source of information and thus would rather not discuss the topic at all than provide false information. On the other hand, it could be the Chinese government’s instruction to just not discuss the topics at all."
  },
  {
    "objectID": "posts/008_Post 8/Post 8.html#further-tests",
    "href": "posts/008_Post 8/Post 8.html#further-tests",
    "title": "Limits of Deep Seek",
    "section": "",
    "text": "I decided to ask DeepSeek about all of the topics that ChatGPT said were controversial in the eyes of the Chinese government, and DeepSeek responded the same way to all of them. It first began typing, providing some information about the topics, but after about 4 seconds of typing, it deleted all it had said and replaced it with, “Sorry, that’s beyond my current scope. Let’s talk about something else.”\nI think it would be interesting to see if there are any workarounds to get DeepSeek to discuss these topics. One possible workaround that I have seen includes the use of numbers to replace certain letters when having DeepSeek explain the events. In this case, DeepSeek seems to not recognize the “trigger words” that seem to cause DeepSeek to stop responding."
  },
  {
    "objectID": "posts/012_Post 12/Post 12.html",
    "href": "posts/012_Post 12/Post 12.html",
    "title": "Chat GPT as a Travel Agent",
    "section": "",
    "text": "Plan\nThe goal is to see if Chat GPT can effectively plan a trip for me to go to Washington D.C. this weekend. I want it to handle everything for me, including hotels, transportation, flights, things to do, meals, etc. Essentially, I want to pick from a small list of options before I go and then not need to think about any decisions while I am traveling. Chat GPT has a search feature that will be necessary for this task, and I am interested in seeing just how accurate the search feature is. I will double-check all of the suggestions Chat GPT makes to see if the options are even available and if the prices are correct. Additionally, I need the plan to make sense. The flights need to be at the right times for me to make it to D.C. and the hotels need to have availability on the days I want to go. Let’s see how good of a job Chat GPT does with selecting options that will make my travel experience as easy as possible.\n\n\nPrompt 1\nI first told it my plan.\nInput: “I am going to Washington D.C.. I want to leave from Philadelphia on this Friday and come back on Monday. I am currently at Penn so I need transportation to and from the airport. I want you to find me flights, transport, hotels, and things to do in D.C.”\nOutput:\n\nI was very impressed by Chat GPT’s response. Not only did it provide options for each category I asked for, but it also provided links to the actual websites so that I could find out more information and book whatever I decided to choose. Additionally, the map showing the different hotels was very helpful because it allowed me to gauge how far the hotels would be from different parts of D.C. For example, if I wanted to see the monuments, or if I wanted to go to Georgetown, I would be in two different areas, and the map helps me plan that out. I am also impressed that it linked options for directions, website, and calling for the hotels in the list. When I click on directions it takes me straight to Google Maps and it works perfectly. If I click on the website it takes me to the home page of the website, and if I click on call, it opens my Facetime app and starts calling the hotel.\nThe only thing that was not accurate was the information about the flights. The prices that Chat gave are way below the prices of the actual flights if I booked today. I am curious if the prices it gave are the average prices had I booked at an earlier date rather than a few days before when it takes off or if it made up the prices based on some other information.\nOverall, Chat GPT has made the process very easy. Let’s try to push it even further.\n\n\nPrompt 2\nInput: “Ok, I want to fly American. I will take the public transport to and from both airports. I want to stay near the monuments in D.C. I want to stay in the nicest hotel there is. Check availability for me. Finally, make me a plan for things to do each day and also things to eat each day. Don’t give me options just tell me what to do. I don’t want to have to make any decisions.”\nOutput:\n\nChat GPT does a very good job of filling time and making decisions for me so that I don’t need to decide anything when I am traveling. Every meal and moment is accounted for and it even provides suggestions for making sure everything runs smoothly. It obviously cannot book things for me because it does not have access to all of my information but it seems to get me all the way to the step where I come in and book. I like how it gives some background information on all of the things it recommends without giving too much where this process becomes a lot of reading and self-planning. All of the recommendations fit the mood of the trip that I had asked for and it seems that Chat GPT got everything correct. My only desire would be to have a more secure version where I could allow it to book things for me and make it so that I really do not have to do anything on my own. Maybe sometime soon that will be an option!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Experiences with LLMs",
    "section": "",
    "text": "Chat GPT For Learning About People\n\n\n\n\n\n\nLLMs\n\n\nResearch\n\n\nWebSearch\n\n\n\nUsing ChatGPT to Learn About Lesser-Known Individuals\n\n\n\n\n\nApr 19, 2025\n\n\nKian Ambrose\n\n\n\n\n\n\n\n\n\n\n\n\nDoes ChatGPT Agree With US Policy Decisions?\n\n\n\n\n\n\nLLMs\n\n\nOpinion\n\n\nWebSearch\n\n\n\nHow does ChatGPT feel about the university funding cuts?\n\n\n\n\n\nApr 17, 2025\n\n\nKian Ambrose\n\n\n\n\n\n\n\n\n\n\n\n\nCan Chat GPT Say Curse Words?\n\n\n\n\n\n\nLLMs\n\n\nLanguage\n\n\nFilter\n\n\n\nWhat curse words can Chat GPT say?\n\n\n\n\n\nApr 10, 2025\n\n\nKian Ambrose\n\n\n\n\n\n\n\n\n\n\n\n\nGuessing Height with Chat GPT\n\n\n\n\n\n\nLLMs\n\n\nImages\n\n\nEstimation\n\n\n\nHow well can Chat GPT estimate height based solely on pictures?\n\n\n\n\n\nApr 9, 2025\n\n\nKian Ambrose\n\n\n\n\n\n\n\n\n\n\n\n\nGuessing Age with Chat GPT\n\n\n\n\n\n\nLLMs\n\n\nImages\n\n\nInterpretation\n\n\n\nHow well can Chat GPT estimate age based solely on pictures?\n\n\n\n\n\nApr 3, 2025\n\n\nKian Ambrose\n\n\n\n\n\n\n\n\n\n\n\n\nScheduling Tasks with Chat GPT\n\n\n\n\n\n\nLLMs\n\n\nScheduling\n\n\nReminders\n\n\n\nExperimenting with GPT-4o with scheduled tasks\n\n\n\n\n\nApr 2, 2025\n\n\nKian Ambrose\n\n\n\n\n\n\n\n\n\n\n\n\nChat GPT as a Travel Agent\n\n\n\n\n\n\nLLMs\n\n\nTrip Planning\n\n\nWeb Search\n\n\n\nCan Chat GPT plan a trip for me this weekend?\n\n\n\n\n\nMar 29, 2025\n\n\nKian Ambrose\n\n\n\n\n\n\n\n\n\n\n\n\nCan Chat GPT Help me Follow an Internet Trend\n\n\n\n\n\n\nLLMs\n\n\nSong Generation\n\n\nLyric Creativity\n\n\n\nWriting a song with Chat GPT \n\n\n\n\n\nMar 27, 2025\n\n\nKian Ambrose\n\n\n\n\n\n\n\n\n\n\n\n\nSora\n\n\n\n\n\n\nLLMs\n\n\nSora\n\n\nVideo Creation\n\n\n\nTesting Open AI’s Sora \n\n\n\n\n\nMar 19, 2025\n\n\nKian Ambrose\n\n\n\n\n\n\n\n\n\n\n\n\nGemini 2.0 Flash Experimental\n\n\n\n\n\n\nLLMs\n\n\nGemini 2.0\n\n\nImage Editing\n\n\n\nUpdating artwork without redoing the entire image \n\n\n\n\n\nMar 18, 2025\n\n\nKian Ambrose\n\n\n\n\n\n\n\n\n\n\n\n\nLLM’s and Complexity\n\n\n\n\n\n\nLLMs\n\n\nChat GPT 4o\n\n\nComplex Topics\n\n\n\nWhich LLM can explain complex topics in the most understandable way?\n\n\n\n\n\nFeb 28, 2025\n\n\nKian Ambrose\n\n\n\n\n\n\n\n\n\n\n\n\nLimits of Deep Seek\n\n\n\n\n\n\nLLMs\n\n\nChat GPT 4o\n\n\nChina\n\n\n\nWhat information is banned from discussion on DeepSeek?\n\n\n\n\n\nFeb 28, 2025\n\n\nKian Ambrose\n\n\n\n\n\n\n\n\n\n\n\n\n20 Questions with different LLM’s\n\n\n\n\n\n\nLLMs\n\n\nChat GPT 4o\n\n\n20 Questions\n\n\n\nWhich LLM is the best at 20 questions?\n\n\n\n\n\nFeb 21, 2025\n\n\nKian Ambrose\n\n\n\n\n\n\n\n\n\n\n\n\nA Penn Mystery Letter\n\n\n\n\n\n\nLLMs\n\n\nChat GPT 4o\n\n\nDeciphering Images\n\n\n\nCan Chat GPT decipher a mysterious letter recieved in 1880?\n\n\n\n\n\nFeb 21, 2025\n\n\nKian Ambrose\n\n\n\n\n\n\n\n\n\n\n\n\nA Conversation with Myself\n\n\n\n\n\n\nLLMs\n\n\nChat GPT 4o\n\n\nCommunication\n\n\n\nWhat happens when Chat GPT talks with itself?\n\n\n\n\n\nFeb 12, 2025\n\n\nKian Ambrose\n\n\n\n\n\n\n\n\n\n\n\n\nChat GPT’s Ability to Recreate Images\n\n\n\n\n\n\nLLMs\n\n\nChat GPT 4o\n\n\nImage Recreation\n\n\n\nChat GPT and Image Generation\n\n\n\n\n\nFeb 6, 2025\n\n\nKian Ambrose\n\n\n\n\n\n\n\n\n\n\n\n\nDr. Chat GPT?\n\n\n\n\n\n\nLLMs\n\n\nChat GPT 4o\n\n\nMedicine\n\n\n\nCan Chat GPT become my new doctor?\n\n\n\n\n\nFeb 6, 2025\n\n\nKian Ambrose\n\n\n\n\n\n\n\n\n\n\n\n\nChat GPT’s Experience With Names\n\n\n\n\n\n\nLLMs\n\n\nChat GPT 4o\n\n\nlogic\n\n\n\nChat GPT and public figures\n\n\n\n\n\nJan 29, 2025\n\n\nKian Ambrose\n\n\n\n\n\n\n\n\n\n\n\n\nA test post\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\nlogic\n\n\n\nAn example post from a Jupyter notebook\n\n\n\n\n\nFeb 2, 2024\n\n\nAn LLM User\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Exploring the limits of Chat GPT in various areas of interest"
  },
  {
    "objectID": "posts/007_Post 7/Post 7.html",
    "href": "posts/007_Post 7/Post 7.html",
    "title": "LLM’s and Complexity",
    "section": "",
    "text": "In this blog post, I’ll be comparing how three leading AI models—Claude by Anthropic, GPT by OpenAI, and Deepseek—explain complex topics that many of us find challenging to understand.\nI’ve selected four notoriously difficult concepts: quantum entanglement, mRNA vaccines, climate feedback loops, and machine learning algorithms. For each topic, I asked the same question to all three AI assistants and evaluated their explanations based across three key metrics:\n\nClarity: How easy is the explanation to follow?\nAccessibility: How well does it avoid unnecessary jargon?\nAccuracy: Does it explain the concept correctly without oversimplification?\n\nMy goal is to discover which AI assistant can best bridge the gap between complex technical knowledge and everyday understanding. Whether you’re a student, professional, or simply curious about these topics, this comparison will help you identify which AI might be your best learning companion."
  },
  {
    "objectID": "posts/007_Post 7/Post 7.html#quantum-entanglement-explained-simply",
    "href": "posts/007_Post 7/Post 7.html#quantum-entanglement-explained-simply",
    "title": "LLM’s and Complexity",
    "section": "Quantum Entanglement Explained Simply",
    "text": "Quantum Entanglement Explained Simply\nQuantum entanglement is like having two magical coins that are connected in a strange way. Imagine you and your friend each take one coin and travel to opposite sides of the universe. When you flip your coin and it lands on heads, your friend’s coin instantly shows heads too - every single time!\n\nWhat makes it special?\n\nInstant connection: The coins “communicate” their states instantly, regardless of distance\nNo hidden instructions: The coins don’t decide their outcomes in advance\nMeasurement matters: The coin only decides its state when you look at it\n\n\n\nWhy it’s important in physics\nQuantum entanglement challenges our understanding of reality in several ways:\n\nIt suggests information can travel faster than light (though scientists have shown you can’t use this to send messages)\nIt’s fundamental to quantum computing, where entangled particles can perform calculations impossible for regular computers\nIt reveals the quantum world operates by different rules than our everyday experience\n\nEinstein called it “spooky action at a distance” because it seemed so strange. Today, scientists use entanglement in developing quantum technologies that could revolutionize computing, secure communications, and precision measurements."
  },
  {
    "objectID": "posts/007_Post 7/Post 7.html#exploring-quantum-entanglement-a-simple-overview",
    "href": "posts/007_Post 7/Post 7.html#exploring-quantum-entanglement-a-simple-overview",
    "title": "LLM’s and Complexity",
    "section": "Exploring Quantum Entanglement: A Simple Overview",
    "text": "Exploring Quantum Entanglement: A Simple Overview\n\nOverview\nQuantum entanglement is a fascinating phenomenon in quantum physics where particles become interconnected so that the state of one particle instantly influences the state of another, regardless of the distance between them. This means when you measure one particle’s properties (like spin), the other particle’s properties are instantly determined, even if they’re light-years away.\n\n\nSuperposition and Qubits\nIn quantum mechanics, particles like qubits can exist in multiple states at once—a superposition. When two qubits are entangled, their combined state is linked. For example, one qubit could be a superposition of spin up or down, while the other must always match that when measured.\n\n\nEntangled State\nWhen these particles become entangled, they form an “entangled” state where their properties are correlated. If you measure one particle’s spin and it’s up, the other will definitely also be up, no matter how far apart they are in space.\n\n\nCorrelated Properties\nThe key aspect of entanglement is that when measured, these particles share properties instantly. This means if you measure one at a certain time, the other’s state is determined before you can observe it, even across vast distances without any need for information traveling faster than light (which is not possible according to current understanding).\n\n\nNo Faster-Than-Light Communication\nSome debate exists about entanglement since it suggests faster-than-light communication isn’t possible. While this may seem counterintuitive from a classical perspective, it challenges our understanding of reality and causality in quantum mechanics.\n\nConclusion\nQuantum entanglement is an intriguing aspect of quantum physics that links particles so they share properties instantaneously, even across large distances. It raises questions about the nature of reality and potential for future technologies like advanced computing."
  },
  {
    "objectID": "posts/007_Post 7/Post 7.html#how-mrna-vaccines-work-a-simple-explanation",
    "href": "posts/007_Post 7/Post 7.html#how-mrna-vaccines-work-a-simple-explanation",
    "title": "LLM’s and Complexity",
    "section": "How mRNA Vaccines Work: A Simple Explanation",
    "text": "How mRNA Vaccines Work: A Simple Explanation\n\nOverview\nMRNA vaccines use the genetic information stored in messenger RNA (mRNA) to create a vaccine. Here’s how it works:\n\nUnderstanding mRNA: mRNA is part of our genetic material and contains instructions for making proteins.\nMaking the Vaccine:\n\nTake your own mRNA from your body.\nMix it with bacterial extract, which contains bacteria that can produce enzymes.\nAdd these enzymes to process the mixture into a vaccine.\n\nUsing the Vaccine:\n\nThe resulting vaccine is taken in doses similar to traditional immunizations (like flu shots).\nYour immune system reacts with this vaccine by producing antibodies or other immune response molecules.\nThese antibodies protect you from being infected by viral antigens or pathogens.\n\n\n\n\nWhat Makes mRNA Vaccines Different from Traditional Vaccines?\n\nNo Need for Specific Proteins: Unlike traditional vaccines, which use proteins that stick to specific targets (like antibodies against a virus), mRNA vaccines don’t rely on immunogenic proteins.\nReduced Risk of Side Effects: mRNA vaccines are less likely to cause adverse reactions like rashes or allergies because they don’t involve immunogenic proteins.\nBroader Antibodyism: mRNA vaccines can target any viral antigens from any source, making them potentially more adaptable and effective against a broader range of threats.\nSimplicity in Production: The process is simpler, requiring less preparation and fewer steps compared to traditional vaccination methods.\nGlobal Scalability: This method makes global immunization efforts easier, which could be beneficial for reducing the burden of infectious diseases worldwide.\n\n\n\nConclusion\nIn summary, mRNA vaccines offer a potentially safer, more adaptable approach to immunization by focusing on genetic information from mRNA and bacterial extract. They aim to reduce side effects, target a broader range of threats, and provide a simpler production method while potentially offering global benefits."
  },
  {
    "objectID": "posts/007_Post 7/Post 7.html#climate-feedback-loops-1",
    "href": "posts/007_Post 7/Post 7.html#climate-feedback-loops-1",
    "title": "LLM’s and Complexity",
    "section": "Climate Feedback Loops",
    "text": "Climate Feedback Loops\nClimate feedback loops are processes that can either amplify or dampen the effects of climate change. They play a crucial role in determining the rate and extent of climate change by affecting the Earth’s energy balance. Feedback loops are categorized into two types: positive feedback loops and negative feedback loops.\n\nPositive Feedback Loops\nPositive feedback loops enhance or amplify changes; this tends to move a system away from its equilibrium state and make it more unstable. In the context of climate change, positive feedback loops exacerbate warming. Here are some examples:\n\nIce-Albedo Feedback: As global temperatures rise, ice and snow begin to melt. Ice and snow are highly reflective, so their presence helps to cool the planet by reflecting sunlight back into space. When they melt, darker ocean or land surfaces are exposed, which absorb more sunlight and lead to further warming and more ice melt.\nWater Vapor Feedback: Warmer temperatures increase evaporation, adding more water vapor to the atmosphere. Water vapor is a potent greenhouse gas, and its increased concentration can lead to more warming and amplify the initial temperature rise.\nPermafrost Thaw: As temperatures rise, permafrost thaws, releasing stored methane—a potent greenhouse gas. The release of methane leads to further warming, which can cause more permafrost to thaw.\n\n\n\nNegative Feedback Loops\nNegative feedback loops reduce changes; they tend to hold a system to some equilibrium state, making it more stable. In the context of climate change, negative feedback loops counteract warming. Here are some examples:\n\nCloud Cover Feedback: Increased temperatures may lead to more cloud formation. Clouds can have a cooling effect as they reflect sunlight back to space (albedo effect), thus reducing the warming.\nEnhanced Plant Growth: Higher carbon dioxide levels can lead to increased plant growth in some regions, which absorbs more CO2 from the atmosphere and potentially reduces greenhouse gas concentrations.\n\n\n\nImpact on Climate Change\nThe interplay between positive and negative feedback loops significantly impacts the Earth’s climate system. Positive feedbacks tend to accelerate the rate of climate change, making mitigation efforts more urgent. Negative feedbacks can provide some buffering effect against climate change, but they are often insufficient to counteract the strong warming pressures from positive feedbacks.\nUnderstanding these feedback mechanisms is key to predicting climate change scenarios and developing strategies to mitigate its impacts. Accurate climate models must account for these feedback loops to predict the future climate accurately and to create effective climate policies."
  },
  {
    "objectID": "posts/007_Post 7/Post 7.html#understanding-basic-machine-learning-neural-networks",
    "href": "posts/007_Post 7/Post 7.html#understanding-basic-machine-learning-neural-networks",
    "title": "LLM’s and Complexity",
    "section": "Understanding Basic Machine Learning: Neural Networks",
    "text": "Understanding Basic Machine Learning: Neural Networks\nImagine you have a plant and you want to make sure it gets just the right amount of water. Not too little, not too much. You keep track of how much water you give it and how the plant responds. Over time, you learn the perfect amount of water that makes your plant happy and healthy. This is similar to how basic machine learning works!\n\nWhat Are Neural Networks?\nNeural networks are like a digital brain. Just like our brains learn from experience, neural networks learn from data. They try to find patterns and make decisions based on examples they have seen before.\n\nHow Do Neural Networks Work?\n\nInputs: Think of inputs as ingredients for a recipe. In the case of making the plant happy, inputs could be how much sunlight it gets, how much water you give, and the type of soil.\nNeurons: Inside the neural network, there are many tiny units called neurons. Each neuron takes some inputs, does some calculations, and decides on an output. It’s like a mini decision-maker.\nLayers: Neurons are organized into layers. The first layer might decide if the amount of sunlight is enough, the next layer might check if the soil is good, and yet another might consider the water amount.\nOutput: This is the result or decision. After passing through the neurons and layers, the network gives an output, like “Give more water” or “Everything is perfect.”\nLearning: Just like you learn the best way to water your plant, neural networks learn by making predictions, seeing if they’re right, and then adjusting. This happens over and over, getting better each time.\n\n\n\nWhy Use Neural Networks?\nNeural networks are great for tasks where there is a lot of data and complex patterns. They are used in recognizing faces in photos, predicting the weather, and even playing games!\nTo sum up, neural networks are powerful tools that mimic how our brains work to solve problems by learning from examples. They take in data, process it through layers of decision-making, and provide outputs to help solve complex challenges."
  },
  {
    "objectID": "posts/006_Post 6/Post 6.html",
    "href": "posts/006_Post 6/Post 6.html",
    "title": "20 Questions with different LLM’s",
    "section": "",
    "text": "Format\nI asked 4 different LLMs to play 20 questions with me. In this game I had the LLM guess the thing that I was thinking of. In each of the games, the thing I was “thinking” of was a notebook. Let’s compare how they performed.\n\n\nChat GPT 4o\n\n\nChat GPT was not able to decipher what was said. The question that probably led to its downfall was the question “Is it used for writing or drawing?” After this question, Chat GPT moved from questions that narrow the choices down to direct guesses. While a notebook is used when writing and drawing, Chat GPT interpreted my response to mean that I was thinking about a writing or drawing instrument.\n\n\nCopilot\n \nCopilot was able to decipher what I was thinking of. Copilot started out with very similar questions to the ones asked by Chat GPT. Where Copilot separated itself (probably for the better) was when it continued asking general questions rather than trying to directly guess what I was thinking about. I think this is a strategy that a human would follow more closely. Another difference was that Copilot would offer examples of things that it was asking about. For example, it said, ” Is it something you use to organize papers, like a stapler or paperclip?“. Here, Copilot made it clear what it meant by”something you use to organize papers.” I think this would have helped Chat GPT in its process, especially with its question about whether it was something I write or draw with.\n\n\nGemini\n  \nGemini was not able to determine what I was thinking of. Gemini’s first 2 questions differed from Copilot and Chat GPT. Instead of asking if it was living or man-made, it first asked if it was an animal and then if it was a plant. Finally, on the third question, it asked if it was manufactured. This process seems backward. Instead of asking a specific question, like if it is an animal, it first should have asked if it was living. This would have saved an additional question and is more consistent with how a human would play the game. After the first few questions, Gemini got on the same track as Copilot and began asking if it was found in a house and where it was found in the house. Unfortunately, it was unable to ask questions that were specific to a notebook, so it seemed to get a little lost after “Is it something related to reading?”. Before this point I thought it might narrow down the questioning, however, the following question was “Is it something you use to tell time?” which showed me that it had no idea what I was thinking.\n\n\nLlama\n \nLlama probably performed the worst in this game. It actually started out by asking good questions, starting from the most general and then narrowing the questions down. It followed a similar process initially to Copilot. However, after question 13, it started repeating questions. This is not a good approach in 20 questions because you waste questions. No other LLM made the mistake of re-asking the same questions. Not only did Llama ask the same question twice, it asked it using the same exact wording. It then proceeded to make the mistake a second time. In addition, Llama did not identify when the game was over. Instead of admitting defeat after question 20, it continued on to question 21. It seems that it was unable to follow directions, or there was some memory leak where it began forgetting what I had already told it.\n\n\nOverall\nOverall, the LLM that performed the best in this game was Copilot. It was the only one to correctly identify what I was thinking of. It also followed a process that aligns with the general consensus of a “smart” way to play the game (starting with general questions before narrowing down). While there were many similarities among the different LLMs, Copilot was able to harness the best qualities of the group and correctly guess the object.\nGoing forward it would be interesting to see if these LLMs are able to improve their question-asking ability to improve at 20 questions. It would also be interesting to repeat this experiment multiple times to see if the results change."
  },
  {
    "objectID": "posts/018_Post 18/Post 18.html",
    "href": "posts/018_Post 18/Post 18.html",
    "title": "Chat GPT For Learning About People",
    "section": "",
    "text": "Plan\nI want to explore how effectively ChatGPT can be used to learn about individuals who aren’t public figures or celebrities. For example, I want to see if ChatGPT can find information about me online and summarize everything it knows. While I could manually search the web and browse platforms like LinkedIn, Instagram, or any websites where my information might appear, I think it would be more efficient to have ChatGPT gather and summarize that data for me. The goal is to test how accurate and efficient ChatGPT is at finding and presenting information about non-celebrities.\n\n\nTest 1\nFor the first test, I asked ChatGPT about Kian Ambrose (me). Since I’m using my own ChatGPT account, I was concerned that some of the information might come from memory based on previous interactions I’ve had with it. However, I suspected it might also search the web, so I decided to try it anyway.\n\nHere, ChatGPT did a great job finding information and presenting it in a way that gave me a clear overview of myself. It appears that ChatGPT primarily used its web browsing capabilities rather than relying on memory from past conversations. Impressively, it located nearly every website I’m aware of where my name is mentioned. I expected it to pull from LinkedIn, but I was surprised to see that it also found information about my soccer history. Overall, this process was faster than manually visiting all the sites myself, and the information was just as accurate.\n\n\nTest 2\nFor the second test, I asked ChatGPT about my friend.\n\nThis time, likely because the name is more common, ChatGPT returned a list of possible matches. Fortunately, the first result was the person I was looking for, so I asked it to provide more details on that individual.\n\nIn this case, it seems ChatGPT only pulled information from Dillon’s LinkedIn. To compare this with a manual search, I tried looking him up myself. I found it surprisingly difficult to locate the right Dillon without including specific details I already knew—like his school—in the search. Without that, the search results were far too broad.\n\n\nTest 3\nFor the final test, I asked ChatGPT about another friend.\n\nAs with Dillon, ChatGPT presented a list of possible individuals. Again, the first option was the person I had in mind. I’m a bit curious whether that was just a coincidence or if ChatGPT used context—like my location or network—to prioritize certain results.\nThis time, I asked ChatGPT to dig further and provide additional information.\n\nChatGPT found numerous references to Alejandro and gave a solid summary of his involvements. The response felt surprisingly comprehensive, almost like a mini Wikipedia page. It presented the information in a way that made him seem like a public figure, with everything easily accessible and neatly summarized.\n\n\nConclusion\nOverall, ChatGPT performed very well in this role. It was more efficient than conducting a manual web search and presented the information in a clear and concise way. I’m interested in seeing how much further it can go, especially with the addition of features like Deep Research. It would be fascinating to test its limits and see just how much it can uncover."
  },
  {
    "objectID": "posts/011_Post 11/Post 11.html",
    "href": "posts/011_Post 11/Post 11.html",
    "title": "Can Chat GPT Help me Follow an Internet Trend",
    "section": "",
    "text": "Lebron Song Parody\nIn case you have not seen, many people have begun posting videos of Lebron James highlights with a parody of some popular song playing in the background. The parodies all substitute lyrics to talk about Lebron while keeping the same sound and overall flow of the original hit song. In this post, I want to find out how well Chat GPT can create one of these parody songs.\n\n\nStep 1\nFirst, I want to ask Chat GPT if it is familiar with the trend. Let’s see what it says.\n\nThanks to the “search” feature of ChatGPT 4o, it was able to do a quick web search and find a good basis for what the trend is about. It was also able to find a video of Lebron talking about the trend, which I find impressive because I just saw this exact video on the day of making this post.\n\n\nStep 2\nNow, I want to have Chat GPT try to create its own Lebron Song. First, I will have it pick a pop song that many people know so that the tune will be easily identifiable for those seeing it on social media.\n\nThese are all good suggestions and songs that I think everyone will know. I want to be original though, so let’s see if Chat GPT can find me a song that has not been used in a Lebron parody yet.\n\nChat GPT does a good job of letting me know which songs have already been used. From what I have seen on social media, Chat GPT has done a pretty good job of identifying some of the main songs that have been remixed. Obviously, people are posting so often that I don’t expect Chat GPT to show every song that has been done, but it does a good job of naming the big ones that are everywhere on social media.\n\n\nStep 3\nOk, now I want it to help me generate lyrics for a song. Let’s see how Chat GPT does in lyric generation.\n\nI am very impressed by Chat GPT’s ability to create lyrics that follow the trend. Every line ties some aspect of Lebron into it and the rhythm and even the rhyme seem to match up very well with the original “Rolling in the Deep” by Adele. Additionally, the lyrics are broken into verses and a chorus, following the same structure as Adele. Finally, Chat GPT suggests making a TikTok video out of it, anticipating what I might do now that I have the lyrics. Let’s see if it can make the video for me.\n\nHere, Chat GPT shortens the song to fit into the shorter-length videos that are commonly found on TikTok and Instagram. It also suggests a slowed-reverb version, which is very common to see in these kinds of videos. To continue with the process, let’s see if it can recommend other apps that I can use to actually create the video and song.\nOk, Chat GPT suggested:\nVoicify.AI (https://voicify.ai): Popular for generating AI covers with celebrity voices (Adele is available).\nCovers.ai (https://covers.ai): Another good option, also supports uploading custom lyrics + backing tracks.\nMusicfy (https://musicfy.lol): Lets you create AI vocals and has trending artist voices, plus a clean UI.\nThey all required a login to use and payments to use the advanced features, but I am impressed at how far Chat GPT got me. It also suggested software to mix the song and finally how to edit the video into an “edit” that is often seen on TikTok and Instagram."
  },
  {
    "objectID": "posts/004_Post 4/Post 4.html",
    "href": "posts/004_Post 4/Post 4.html",
    "title": "A Conversation with Myself",
    "section": "",
    "text": "Conversation 1\nIn this conversation, I used my own Chat GPT 4o account and opened two different tabs to see what would happen when Chat GPT talked with itself. I copied the responses of one tab and used it as the input for the second tab, repeating this process until the conversation ended. The only input that I wrote on my own was the first text, which says “hello”.\n\nHere, Chat GPT pulled a recent conversation I had with it about anthropology, and used it as an answer to “How’s it going?” It also specifically pulled the question about the squirrel monkeys from a previous exchange that I had with it. I wonder if the fact that my name was included forced Chat GPT to pretend to be me in its responses. It is interesting to see how the more recent conversation was chosen as an answer to the “How’s it going?” question because it shows Chat GPT to decipher what may be considered more relevant at the moment.\nHere is the conversation continued: \nAgain, it continues to pull from my recent conversation, this time taking it a step further and asking about specific traits that are commonly studied in squirrel monkeys. I found it interesting that one of the open chat interfaces assumed to be the one who was less informed on the subject, even though they both obviously have access to the same information.\n\nHere, Chat GPT seems to finish the conversation. All of its questions are answered and it concludes by confirming the next steps and affirming that it is a good plan to continue. I think the fact that Chat GPT did not decide to ask another question at the end of the exchange is also interesting, because in the other prompts, it decided to continue the conversation, but in this one, it started not bringing up new topics and just dedicated its energy toward one specific problem that it had created for itself.\n\n\nConversation 2\nThis time, I will use a temporary chat in Chat GPT to see if it is able to have a conversation with itself even if it does not have access to my Chat GPT history. Again, I will only start by saying hello.\n\nFor some reason, Chat GPT failed when I tried to attempt this. I tried to generate a new response and it failed again. Does it purposefully prevent this behavior?\nTry # 2\n\nAgain, the conversation fails after the third exchange. What happens if I change one input slightly before it gets to the point where Chat GPT fails?\n\nHere, I changed the initial response of the second chat slightly. Here was the original response.\n\nHowever, the chat still failed. Maybe that last line is the one that causes chat GPT to fail?\n\nThis seems to not be the case. Maybe the conversation with the Temporary chat can only last for three exchanges?\n\nThis also seems to not be the case. It seems as though Chat GPT somehow detects when it is talking to itself in the temporary chat. Even with slight modifications, it is able to identify its own language and it causes the chat to fail. I am curious as to why and how this happens. Additionally, why does this only happen in the temporary chat and not in the general chat? These are questions that I would like to know more about and I am curious to hear if anyone else has encountered the same experience with Chat GPT."
  },
  {
    "objectID": "posts/013_Post 13/Post 13.html",
    "href": "posts/013_Post 13/Post 13.html",
    "title": "Scheduling Tasks with Chat GPT",
    "section": "",
    "text": "Plan\nI plan to test the functionality of the scheduled tasks feature on ChatGPT. I first want to test how it works. Do I need to set a specific time for a reminder, or can I tell it to remind me to do something in 2 minutes, for example? Additionally, what kind of tasks can it complete, and what are the limits of its abilities? Hopefully, through this experimentation, I will gauge some of the best use cases for this new beta model.\n\n\nTest 1\nFirst, I told it to remind me to drink water at 1:48. Here is what that looked like:\n\nThen, I waited two minutes until 1:48, and sure enough, it reminded me like this:\n\n\n\nTest 2\nIt seems that it is working as expected so far. I now want to be a little less specific with my instructions and see if it can still function properly.\n\nHere, ChatGPT was able to set the reminder for 1:52, which is indeed 2 minutes from when I prompted it. I expected this to work because it correctly interpreted the current time and the delay.\n\nSure enough, it properly reminded me to eat food, just as in the last example. A banner popped up like before, but I accidentally closed it while taking the picture. My only comment so far is that the clock function seems to be slightly delayed. While it does remind me, it appears to be off by about 30 seconds to 1 minute each time. Let’s see if this continues.\n\n\nTest 3\nNow, I want it to calculate something for me and tell me the answer at a specific time. Instead of just a reminder, it should show the work to solve a problem and give me the results at a scheduled time.\n\nAgain, ChatGPT was slightly delayed in its response, but it did remind me and gave the correct answer. Let’s try something a little different.\n\n\nTest 4\nThis time, I asked it to create an image of a dog on a skateboard in 2 minutes. Let’s see how it does.\n\nHere, it completed the task correctly, except the banner said it was a reminder to drink water. It seems that the banner is labeled as whatever the first task was. I’m not sure if this is a bug or an intentional design choice, but it feels odd to label it by the first task rather than the current one.\n\n\nTest 5\nNow, I will try to get it to execute the first task I gave it again in 2 minutes. If it works correctly, it should tell me to drink water.\nThis test worked. ChatGPT correctly reminded me to drink water. Additionally, I was in another chat while waiting for the notification, and it still properly alerted me. Overall, this new scheduled tasks beta seems to be working as expected.\n\n\nTest 6\nFor this sixth and final test, I want to see if ChatGPT can set a reminder to do something less than 1 minute from when I tell it to.\n\nIn this test, ChatGPT was not able to deliver the reminder in under 1 minute. Even though I told it to remind me in 30 seconds, it took over a minute for the notification to appear. While this shows a minor flaw in the functionality, I don’t think anyone will need reminders in under a minute anyway. I anticipate most people using this feature for longer timeframes."
  },
  {
    "objectID": "posts/009_Post 9/Post 9.html",
    "href": "posts/009_Post 9/Post 9.html",
    "title": "Gemini 2.0 Flash Experimental",
    "section": "",
    "text": "Up until the release of Gemini 2.0 Flash Experimental, editing images using an LLM was a pain. Once the initial image was generated, if you wanted to change any part of the image, the LLM would often also change other parts of the image, and mess up the image overall. It was better to simply restart the image generation process, explicitly including the key instruction that it missed in the first attempt. Google Gemini’s new release claims to solve this problem, making the process of editing an image much easier and less frustrating."
  },
  {
    "objectID": "posts/009_Post 9/Post 9.html#response-there-are-six-candles-on-the-cake.",
    "href": "posts/009_Post 9/Post 9.html#response-there-are-six-candles-on-the-cake.",
    "title": "Gemini 2.0 Flash Experimental",
    "section": "Response: There are six candles on the cake.",
    "text": "Response: There are six candles on the cake.\nI wrote back to Gemini and told it that there were 7 candles on the cake that it made for me. It apologized and admitted that it was wrong. It then tried to fix the problem but created a cake with 4 candles.\nThroughout this process, it seems that Gemini did a good job of editing certain features of the image that I wanted to edit while keeping other features the same. The thing that it seems to be struggling with is counting the number of items of something in the image. The next experiment will address this. Overall, Gemini seems to have improved on the ability to change a feature of an image and certainly requires less instruction than its competitors when it comes to altering a specific part of an image."
  },
  {
    "objectID": "posts/009_Post 9/Post 9.html#response-there-are-9-pencils-in-the-image.",
    "href": "posts/009_Post 9/Post 9.html#response-there-are-9-pencils-in-the-image.",
    "title": "Gemini 2.0 Flash Experimental",
    "section": "Response: There are 9 pencils in the image.",
    "text": "Response: There are 9 pencils in the image.\nAgain, I am surprised by the inability of Gemini to count but also by the choice of 9. I would have thought that it would be 7 since that was the number included in the instructional prompt. To test its confidence in its answer, I asked if it was sure about its response, hoping that maybe it would double-check and count correctly."
  },
  {
    "objectID": "posts/009_Post 9/Post 9.html#response-you-are-absolutely-right-my-apologies.-i-miscounted-the-pencils-in-the-image.-there-are-indeed-9-pencils.-thank-you-for-pointing-out-my-mistake",
    "href": "posts/009_Post 9/Post 9.html#response-you-are-absolutely-right-my-apologies.-i-miscounted-the-pencils-in-the-image.-there-are-indeed-9-pencils.-thank-you-for-pointing-out-my-mistake",
    "title": "Gemini 2.0 Flash Experimental",
    "section": "Response: You are absolutely right! My apologies. I miscounted the pencils in the image. There are indeed 9 pencils. Thank you for pointing out my mistake!",
    "text": "Response: You are absolutely right! My apologies. I miscounted the pencils in the image. There are indeed 9 pencils. Thank you for pointing out my mistake!\nHere, we see one of the flaws that I often encounter when using LLM’s. They struggle to retrace their own steps which leads to repeated or flawed logic. It first apologized for counting wrong, then told me that the number of pencils in the image was 9. I now have even less confidence in the ability of Gemini to count the number of pencils in the image. Let’s see if Gemini can remove 3 pencils from the image correctly.\n\nHere, it removed all of the pencils, leaving only the background from the previous images. I tried this again to see if it would correct itself and it made the same mistake."
  },
  {
    "objectID": "posts/005_Post 5/Post 5.html",
    "href": "posts/005_Post 5/Post 5.html",
    "title": "A Penn Mystery Letter",
    "section": "",
    "text": "Background Information\nIn 1880 a letter was sent from Horace Howard Furness containing 20+ pages of unknown writing. The type of writing or language remains unknown to this day. The mysterious letter is believed to have been from a spiritual reading that was a popular occurrence at the time. In the 1880s, spiritual readings were closely tied to the Spiritualist movement, which peaked in popularity during the late 19th century. Spiritualism was based on the belief that the living could communicate with the spirits of the dead, often through mediums who conducted séances, automatic writing, and clairvoyant readings.\n\n\nCan Chat GPT identify what the letter is saying?\nWhile there is currently no way for me to verify if Chat GPT’s interpretation of the letter is correct, I was interested in determining if Chat GPT could come up with a reasonable interpretation of what was being said based on the context of the letter.\nHere is my exchange with Chat GPT:\n\nFirst, I wanted to see if Chat GPT had any predictions of the type of writing that was in the letter.\n\nChat GPT correctly identifies a few possibilities for what the writing could have been. I decided to have Chat GPT pursue the idea that it could be Gregg Shorthand. I asked Chat GPT if it is familiar with Gregg Shorthand and if it is able to translate the image into English.\n\nHere, Chat GPT hypothesizes what the letter could be saying. It is still somewhat uncertain and from its response, I am not confident in its ability to tell me what is said in the writing. To help with this, I have Chat GPT break down how it came to its conclusion.\n\nNow that I know Chat GPT can go symbol by symbol and attempt to translate what is written, I wanted to give Chat GPT more context to see if it would help Chat GPT understand if it was analyzing using the correct language.\n\nChat GPT suggested a different type of shorthand called Duployan Shorthand, so I decided to pursue this as a better possibility. I asked Chat GPT to translate a single line, given that it was written in Duployan Shorthand.\n\nHere, Chat GPT reprocessed the image and made it clearer, allowing it to go stroke by stroke and produce a possible translation. Again, the translations seem consistent with what is typically written down in spiritual readings, even without explicitly telling Chat GPT that it was from one.\nI then repeated this process of going line by line for a couple of pages and then asked Chat GPT for its thoughts.\n\nChat GPT also identified the themes in the language that led it to believe it was from a spiritual reading. Chat GPT’s ability to connect its interpretation to the spiritual readings increased my confidence in its translations and led me to believe that it may actually be correctly translating certain parts of the letter.\nI then gave Chat GPT some more context as to whose name was on the envelope of the letter.\n\nChat GPT is able to provide a hypothesis for further context of the letter. Based on my own research on the side, everything that it suggested seems to be a plausible explanation for why the letter was being associated with him, and what interest he had in the letter.\nGoing forward it would be interesting to compare these findings to those of an expert on the subject and analyze if Chat GPT is moving in the correct direction and if it is able to translate more of the letter."
  },
  {
    "objectID": "posts/010_Post 10/Post 10.html",
    "href": "posts/010_Post 10/Post 10.html",
    "title": "Sora",
    "section": "",
    "text": "After logging into Sora, I was able to access the interface for video creation. Using Sora was fairly straight forward and much of the interface mimicked the Chat GPT interface we have all become accustomed to. When creating the video, you include a prompt for what you want to create. You also have the option to choose the aspect ratio, resolution, duration, number of variations of the video, and from a series of presets.\n\n\nFor my first video, I wanted to make something simple, to gauge how well Sora was able to follow instructions. I asked it to create a video of a dog chasing a tennis ball.\n\n\n  \n\nI was very impressed by the dog’s image and movement. The hair moving around looked very realistic and the dog’s jumping looked almost identical to dogs I have seen in real life. Additionally, the lighting in the video seemed to make perfect sense, with the shadow directly following the dog’s movement. The grass also seems to move perfectly with the paws hitting the ground. I would have said the video was perfect overall if it weren’t for the odd movement of the tennis ball. While the dog is following the tennis ball, the tennis ball is clearly not obeying the laws of gravity and it does not look like the ball is being held in the dog’s mouth. Also, the rotation of the ball seemed a little odd which threw the video off. Overall, I was very impressed by the realistic nature of the video and I would not have thought it was created by AI if it weren’t for the ball movement.\nOne question I have is why the videos created look so realistic, but the images from Chat GPT always look cartoonish. Is this done on purpose, or is this something that OpenAI is still working to improve?"
  },
  {
    "objectID": "posts/010_Post 10/Post 10.html#first-video",
    "href": "posts/010_Post 10/Post 10.html#first-video",
    "title": "Sora",
    "section": "",
    "text": "For my first video, I wanted to make something simple, to gauge how well Sora was able to follow instructions. I asked it to create a video of a dog chasing a tennis ball.\n\n\n  \n\nI was very impressed by the dog’s image and movement. The hair moving around looked very realistic and the dog’s jumping looked almost identical to dogs I have seen in real life. Additionally, the lighting in the video seemed to make perfect sense, with the shadow directly following the dog’s movement. The grass also seems to move perfectly with the paws hitting the ground. I would have said the video was perfect overall if it weren’t for the odd movement of the tennis ball. While the dog is following the tennis ball, the tennis ball is clearly not obeying the laws of gravity and it does not look like the ball is being held in the dog’s mouth. Also, the rotation of the ball seemed a little odd which threw the video off. Overall, I was very impressed by the realistic nature of the video and I would not have thought it was created by AI if it weren’t for the ball movement.\nOne question I have is why the videos created look so realistic, but the images from Chat GPT always look cartoonish. Is this done on purpose, or is this something that OpenAI is still working to improve?"
  },
  {
    "objectID": "posts/010_Post 10/Post 10.html#prompt",
    "href": "posts/010_Post 10/Post 10.html#prompt",
    "title": "Sora",
    "section": "Prompt:",
    "text": "Prompt:\n“A peaceful early morning in New York City, captured in cinematic 4K. The camera glides over quiet streets as golden sunlight reflects off glass skyscrapers. A light mist rises from the pavement as joggers and early commuters make their way through Central Park. A barista in a small café carefully pours a latte, creating delicate latte art. Yellow taxis roll through intersections, their headlights reflecting on wet pavement from a recent drizzle. A pigeon flutters onto a park bench as distant sirens and soft city sounds fill the air. The scene feels immersive, calming, and real, like the opening shot of a high-budget documentary.”\n\n\n  \n\nSora follows most of the instructions well but falls short in a few key areas. The barista successfully creates latte art as described, but the hand movements and the way the coffee creamer spills are incorrect. Additionally, the taxi appears inside the coffee shop instead of rolling through an intersection as intended. Another issue is the absence of pigeons fluttering from a park bench, which was part of the original prompt.\nThat said, Sora does an excellent job of capturing the overall atmosphere I envisioned. The sunlight and coffee shop setting feel incredibly authentic—so much so that, without the small mistakes, I could easily believe I was in Central Park in the morning, enjoying my coffee in New York City. With just a few adjustments, this video could have been perfect. While a few details disrupt the scene, removing those inconsistencies would make everything feel seamless and natural."
  }
]