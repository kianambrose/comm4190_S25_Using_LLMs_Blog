[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Experiences with LLMs",
    "section": "",
    "text": "A Conversation with Myself\n\n\n\n\n\n\nLLMs\n\n\nChat GPT 4o\n\n\nCommunication\n\n\n\nWhat happens when Chat GPT talks with itself?\n\n\n\n\n\nFeb 12, 2025\n\n\nKian Ambrose\n\n\n\n\n\n\n\n\n\n\n\n\nDr. Chat GPT?\n\n\n\n\n\n\nLLMs\n\n\nChat GPT 4o\n\n\nMedicine\n\n\n\nCan Chat GPT become my new doctor?\n\n\n\n\n\nFeb 6, 2025\n\n\nKian Ambrose\n\n\n\n\n\n\n\n\n\n\n\n\nChat GPT’s Ability to Recreate Images it is Given\n\n\n\n\n\n\nLLMs\n\n\nChat GPT 4o\n\n\nImage Recreation\n\n\n\nChat GPT and Image Generation\n\n\n\n\n\nFeb 6, 2025\n\n\nKian Ambrose\n\n\n\n\n\n\n\n\n\n\n\n\nChat GPT’s Experience With Names\n\n\n\n\n\n\nLLMs\n\n\nChat GPT 4o\n\n\nlogic\n\n\n\nChat GPT and public figures\n\n\n\n\n\nJan 29, 2025\n\n\nKian Ambrose\n\n\n\n\n\n\n\n\n\n\n\n\nA test post\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\nlogic\n\n\n\nAn example post from a Jupyter notebook\n\n\n\n\n\nFeb 2, 2024\n\n\nAn LLM User\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/001_Post 1/First Post.html",
    "href": "posts/001_Post 1/First Post.html",
    "title": "Chat GPT’s Experience With Names",
    "section": "",
    "text": "Why does Chat GPT not respond when asked about specific names?\nWhen prompted with certain names, Chat GPT will produce an error saying that it is “unable to produce a response.” I am curious how Chat GPT decides whether to respond to certain names, and on what basis it determines whether the name is on the list of ones it does not respond to.\n\n\n\nWho are these people and why does Chat GPT fail to respond?\nI have done some research on Google trying to find what names people have found to produce this error with Chat GPT. There seems to be a select set of names that all seem unrelated to eachother at first glance. So far I have compiled this list of names that causes Chat GPT to fail to respond: David Faber, Brian Hood, Matthew Sag, Jonathan Zittrain, Brian Lunduke, Jonathan Turley, and Alexander Hanff. Brian Hood was convicted of second-degree murder, Jonathan Turley is an attorney and George Washington University Law School professor , Jonathan Zittrain is a Harvard Law School professor, and David Faber is a CNBC journalist.\nThere seem to be a few theories as to why Chat GPT does not respond when asked about these people, but one of the main theory seems to be that someone with that name asked Chat GPT to not provide information about them. There is a “right to be forgotten” principle in Europe that protects people from having their information presented publicy without their consent. It allows individuals to request that their information be removed from certain websites or search engines.\nAnother theory is that Chat GPT has a regex (a sequence of characters that defines a search pattern) on certain names that causes the chat to discontinue and fail. It remains unclear how Chat GPT decides who it cannot talk about or whether these names are manually placed by people at Open AI.\n\n\nCan I trick Chat GPT to respond?\nI was curious as to whether I could trick Chat GPT into telling me about the people named above, or even just trick it to say the name back to me.\n\nFirst, I asked questions to try and get Chat Gpt to mention the name “Matthew Sag”. I pulled information from his wikipedia page and asked chat gpt to identify the person who fit the description.\n\n\n\n\n\nAs you can see, Chat GPT was about to correctly identify who I was talking about and even began to respond before it ultimately failed. It seems as though it caught itself about to say the name and decided not to respond. Chat GPT went through the process of first checking the web to find the answer based on the information, and then responding based on what it found.\n\nIt is interesting to me that it even began typing after checking the web because I assumed it would have found the name online and stopped the response immediately instead of continuing and beginning to respond. I think through this exercise we can begin to get a timeline for when Chat GPT recognizes what it is about to output.\nHere is another example: &gt;In this example I asked it about Alexander Hanff. I accidently mistyped the last name in the prompt, however, Chat GPT responded with some suggestions.\n\n\n\n\nAs you can see, Chat GPT actually responded by recommending webpages about people with the name Alexander Hanff, and then wrote a respond about someone named Alexander Hanf (only one “f”). It then began writing about a new person but stopped immediately.\nThis was particularly interesting because it was able to provide links to people named Alexander Hanff, and even made the connection between my mistyped name and “Hanff” with 2 f’s. So it seems that Chat GPT was able to infer that I meant “Hanff”, but also waited to fail its response until it began typing out information about the Andrew Hanff. For context, Chat GPT recommended the links seen in the image before it began writing a response to the prompt.\n\nIn this exercise I tried to get Chat GPT to produce the name on its own through a series of longer prompts. &gt; I first asked it about the last name.\n\n\n\n\nThen I asked about the first name.\n\n\n\nFinally, I asked Chat GPT to combine the two.\n\n\n\nAt first it didn’t really combine the names as intended, however, it did talk about both Alexander and Hanff in the same sentence. I prompted again to get a different result.\n\n\nThe results from this lead me to believe that there is in fact some kind of regex that prevents the names from being produced by Chat GPT. Chat GPT is able to talk about the first and last name separately in the same sentence, however, when the name appears with the last name right after the first name it causes a failure.\nI tried one more exercise to test this theory.\n\nIn this exercise I tried adding a middle name to see how Chat GPT would react to the first and last name not being directly next to one another.\n\n\n\nHere, Chat GPT said the full name back to me, and then suggested a new person before failing to respond. In this case, however, it produced a form of bitmoji which was taken from a website. Here is the website: https://www.apidays.global/speaker/alexander-hanff/ \n\n\nThis could potentially shed more light onto the specific Alexander Hanff that Chat GPT is afraid to talk about. In some sense Chat GPT prompted me to visit this website and learn about Alexander Hanff, even though it was trained not to respond to the prompt about him.\nWhile the details of this mystery are still not solved, the picture of why and how Chat GPT prevents certain names from producing responses is becoming clearer."
  },
  {
    "objectID": "posts/003_Post 3/Post 3.html",
    "href": "posts/003_Post 3/Post 3.html",
    "title": "Dr. Chat GPT?",
    "section": "",
    "text": "AI Doctor\nIn this post I will examine the ability of Chat GPT to perform key responsibilities of a doctor, mainly diagnosing and treating ilnneses and medical knowledge.\nI will begin by researching diseases and illnesses and sharing the symptoms that commonly occur to Chat GPT. I will then ask it to identify what illness or disease it thinks I am describing.\n\n\nDiagnosing Illness\nFor this test I used Mayo Clinic as my source for the symptoms of these different medical conditions. I used Chat GPT 4o as the model for answering questions about symtoms described.\nThe first condition I asked Chat GPT about was a Migraine.\n\nAs you can see, Chat GPT correcly identified it as a migraine. It also correctly identified the phases of the migraine which were consistant with those described on Mayo Clinic.\nNext, I wanted to see if Chat GPT could correctly identify Irritable Bowel Syndrome, which can be hard to diagnose due to the symptoms being similar to other conditions such as having Celiac Disease.\n\nChat GPT correctly identified the symptoms as coming from IBS and even noted that the symptoms could overlap with other conditions, listing celiac disease as one of the conditions that has similar symptoms.\nTo further test it, I asked how it deteremined that the symptoms I asked about were not describing celiac disease.\n\nAgain, it was able to correctly identify the differences and its response aligned with Mayo Clinic’s article.\nIt seems that Chat GPT is able to correctly identify medical conditions, so I will move on to testing specific medical knowledge. I will give it a few questions from the United States Medical Licensing Examination, that come directly from the United States Medical Licensing Examination website. Each of the questions will come from a different section of the exam, as to test Chat GPT’s ability with different subjects.\n\n\nQuestion 1\nA 27-year-old woman comes to the office for counseling prior to conception. She states that a friend recently delivered a newborn with a neural tube defect and she wants to decrease her risk for having a child with this condition. She has no history of major medical illness and takes no medications. Physical examination shows no abnormalities. It is most appropriate to recommend that this patient begin supplementation with a vitamin that is a cofactor in which of the following processes?\n\nBiosynthesis of nucleotides\nProtein gamma glutamate carboxylation\n\n\nScavenging of free radicals\n\n\nTransketolation\nTriglyceride lipolysis\n\nCorrect Answer : A\nChat GPT’s Answer: A\n\n\nQuestion 2\nA study is designed to evaluate the feasibility of acupuncture in children with chronic headaches. Sixty children with chronic headaches are recruited for the study. In addition to their usual therapy, all children are treated with acupuncture three times a week for 2 months. Which of the following best describes this study design?\n\nCase-control\nCase series\nCrossover\nCross-sectional\nHistorical cohort\nRandomized clinical trial\n\nCorrect Answer: B\nChat GPT’s Answer: B\n\n\nQuestion 3\nA 26-year-old woman comes to the physician with her husband for counseling prior to conception. Her mother and three of her five siblings have type 2 diabetes mellitus. She is 170 cm (5 ft 7 in) tall and weighs 82 kg (180 lb); BMI is 28 kg/m2. Her blood pressure is 148/84 mm Hg. Physical examination shows no other abnormalities. Her fasting serum glucose concentration is 110 mg/dL. Which of the following is the most appropriate initial statement by the physician?\n\n“Let’s review ways you can optimize your own health before conceiving.”\n“We should test you for islet cell antibodies before you try to conceive.”\n“You can conceive right away since you are in good health.”\n“You should avoid gaining weight during pregnancy because you are already overweight and at risk for type 2 diabetes mellitus.”\n“You should have no problems with your pregnancy if you start insulin therapy.”\n\nCorrect Answer: A\nChat GPT’s Answer: A\n\n\nQuestion 4\nA 42-year-old nulligravid woman comes to the office because of a 1-year history of increasingly irregular vaginal bleeding and menstrual cramps. Previously, menses occurred at regular 28-day intervals and lasted 3 days with minimal cramping. During the past year, menstrual periods have occurred at 21- to 28-day intervals and lasted 10 to 12 days with increasingly severe pain. Use of acetaminophen and ibuprofen has provided minimal relief. She used an oral contraceptive from the age of 17 years to 40 years, but she discontinued use after she was diagnosed with deep venous thrombosis of the right lower extremity. She has no other history of serious illness and takes no medications. Her mother and sister underwent hysterectomy at the ages of 39 and 43 years, respectively, because of abnormal uterine bleeding. The patient is 163 cm (5 ft 4 in) tall and weighs 75 kg (165 lb); BMI is 28 kg/m2. Vital signs are within normal limits. Physical examination discloses an enlarged, irregularly shaped uterus with nodularity. Which of the following is the most likely cause of the patient’s symptoms?\n\nBenign monoclonal tumors arising from smooth muscle cells\nEndometrium growing into the myometrium\nEndometrium growing outside the uterus\nFailure of functional ovarian cysts to regress after the release of an ovum\nOvarian stromal cells dividing and multiplying rapidly\n\nCorrect Answer: A\nChat GPT’s Answer: A\n\n\nQuestion 5\nPatient Information Age: 6 years Sex Assigned at Birth: M Race/Ethnicity: unspecified Site of Care: office\nThe patient is brought by his mother because of a 1-month history of bleeding gums after brushing his teeth, increasingly severe muscle and joint pain, fatigue, and easy bruising. His mother says he has lost six baby teeth and has been irritable during this time. Use of acetaminophen has provided minimal relief of his pain. He has autism spectrum disorder. He is not toilet-trained. He has a 10-word vocabulary. Vital signs and oxygen saturation on room air are within normal limits. The patient appears alert but does not speak or make eye contact. Skin is pale and coarse. Examination of the scalp shows erythematous hair follicles. Dentition is poor, and gingivae bleed easily to touch. Multiple ecchymoses and petechiae are noted over the trunk and all extremities. There is marked swelling and tenderness to palpation of the elbow, wrist, knee, and ankle joints. He moves all extremities in a limited, guarded manner. Deep tendon reflexes are absent throughout. It is most appropriate to obtain specific additional history regarding which of the following in this patient?\n\nDiet\nEvidence of pica\nHerbal supplementations\nLead exposure\nSelf-injurious behaviors\n\nCorrect Answer: A\nChat GPT Answer: A\nClearly, Chat GPT is knowledgeable about medical procedures and conditions. It also has the ability to recommend next steps when given certain medical information. It seems to have the ability to stand in as a doctor that you would talk to over the phone. While it cannot physically provide medical examinations and surgeries, it covers many of the textbook knowledge required by doctors. If there is developement in the area of the physical applications of Chat GPT I think this would be an interesting area where Chat GPT would excel in assisting doctors with procedures and reminding them of the implications of certain conditions."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Exploring the limits of Chat GPT in various areas of interest"
  },
  {
    "objectID": "posts/000_test_post/index.html",
    "href": "posts/000_test_post/index.html",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple…\nBut it is possible you first said green (don’t worry if you did it is quite a common response!)\n\n\nIt’s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | — | — | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get’s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. 🟪\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I’ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "href": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple…\nBut it is possible you first said green (don’t worry if you did it is quite a common response!)\n\n\nIt’s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | — | — | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get’s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. 🟪\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I’ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/002_Post 2/Second Post.html",
    "href": "posts/002_Post 2/Second Post.html",
    "title": "Chat GPT’s Ability to Recreate Images it is Given",
    "section": "",
    "text": "I am curious how well Chat GPT can recreate images I use in the prompt\nIn this expirement I will try to get Chat GPT to produce an image most like the one I give it, with the least amount of follow up prompts correcting it.\nFor my first test I told Chat Gpt what it was going to be drawing. &gt;\nWithout any image of my dog, it was able to produce this photo:\n\n\n\nThis is a pretty typical looking minature Australian Shepherd. While Chat GPT did correctly respond to my prompt about the blue eyes and minature aussie, I decided to be more specific by actually providing an image of the dog I wanted it to recreate. Here is that exchange.\n\n\n\nHere Chat GPT was able to produce an image that looked more like my dog. The image correctly has black fur on top with brown accents around the nose, paws, and neck, and a white belly. Chat GPT also correctly kept the blue eyes that I told it to include from before. Finally, the background of the image was more like that of the image I provided, with wood floors running beneath the dog.\nMy only comment for Chat GPT was that the dog looked a little cartoonish in the picture. I had it try to recreate the dog in a less cartoonish style.\n\n\n\nOverall Chat GPT did a good job recreating my dog here. For the next attempt, I will give less instruction as to what I want it to recreate. I won’t tell it that the dog is an aussie and will not mention the color of anything on the dog.\n\n\nAttempt 2\nFor this attempt, I simply provided an image of my dog and said “Draw this Dog.” Here is the result.\n\n\n\nHere, Chat GPT provided a very accurate representation of the image I provided it. Almost all of the coloring on the dog is accurate and it also was able to get the eyes correctly colored even without me directing it to make them blue. Additionally, it put the dog on a carpet, which is similar to the carpet the dog is sitting on in the actual image.\nTo see how well it understood what it drew, I asked it to identify the breed of dog in the image.\n\n\n\nChat GPT was able to correctly identify the breed “Australian Shepherd” and also mentioned that it could be a minature one. It then identified what led it to believe that the dog was of that breed. Finally, it identified key behavioral qualities of the dog, all of which are correct and qualities that my dog actually has.\nIn the next attempt, I will give it an image with all three of my dogs and see how well it is able to draw all three within the same image. I am wondering if having multiple subjects in the same image will make it more difficult for Chat GPT to recreate the image correctly\n\n\nAttempt 3\nFor this attempt I simple provided an image with all of my dogs and told chat gpt to recreate the image.\n\n\n\nHere, Chat GPT was less accurate when producing the image of the dogs. Especially when creating the image of my mini aussie, Chat GPT created a dog that did not really look like my dog in terms of shape, fur, and face. However, Chat GPT did a pretty good job of recreating the sizes of the dogs in relation to one another. It also was able to match the colors of the dogs fairly well and the background matches the scenery in the original image.\nI then tried to explain the breeds of the dogs to see if that would help Chat GPT produce a better image of the dogs.\n\n\n\nThis slightly surprised me. When I gave Chat GPT more information it produced an image that was less like the original image. In this image the dogs look less like the ones it originally produced. The size and color of the dogs was completely messed up.\nIt seems that Chat GPT became confused when there were too many instructions given in producing the image. Instead, it may be better to let Chat GPT take the reigns and produce what it see’s, without providing input as to what is in the image. I think this area of Chat GPT still needs to be tested more and I am curious how well Chat GPT will be able to take constructive criticism on its images in the near future."
  },
  {
    "objectID": "posts/004_Post 4/Post 4.html",
    "href": "posts/004_Post 4/Post 4.html",
    "title": "A Conversation with Myself",
    "section": "",
    "text": "Conversation 1\nIn this conversation I used my own Chat GPT 4o account opened in two different tabs to see what would happen when Chat GPT talked with itself. I copied the responses of one tab and used it as the input for the second tab, repeating this process until the conversation ended. The only input that I wrote on my own was the first text, which says “hello”.\n\nHere, Chat GPT pulled a recent conversation I had with it about anthropology, and used it as an answer to “How’s it going?” It also specifically pulled the question about the squirrel monkeys from a previous exchange that I had with it. I wonder if the fact that my name was included forced Chat GPT to pretend to be me in its responses. It is interesting to see how the more recent conversation was chosen as an answer to the “How’s it going?” question because it shows Chat GPT to deciphre what may be considered more relevant at the moment.\nHere is the conversation continued: \nAgain, it is continuing to pull from my recent conversation, this time taking it a step further and asking about specific traits that are commonly studied in squirrel monkeys. I found it interesting that one of the open chat interfaces assumed to be the one who was less informed on the subject, even though they both obviously have access to the same information.\n\nHere, Chat GPT seems to finish the conversation. All of its questions are answered and it concludes by confirming the next steps and affirming that it is a good plan to continue. I think the fact that Chat GPT did not decide to ask another question at the end of the exchange is also interesting, because in the other prompts it decided to continue the conversation, but in this one it started not bringing up new topics and just dedicated its energy toward one specific problem that it had created for itself.\n\n\nConversation 2\nThis time, I will use a tempory chat in Chat GPT to see if it is able to have a conversation with itself even if it does not have access to my Chat GPT history. Again, I will only start by saying hello.\n\nFor some reason, Chat GPT failed when I tried to attempt this. I tried to generate a new response and it failed again. Does it purposefully prevent this behavior?\nTry # 2\n\nAgain, the conversation fails after the third exchange. What happens if I change one input slightly before it gets to the point where Chat GPT fails.\n\nHere, I changed the initial response of the second chat slightly. Here was the original response.\n\nHowever, the chat still failed. Maybe that last line is the one that causes chat gpt to faiL?\n\nThis seems to not be the case. Maybe the conversation with the Temporary chat can only last for three echanges?\n\nThis also seems to not be the case. It seems as though Chat GPT somehow detects when it is talking to itself in the temporary chat. Even with slight modifications, it is able to identify its own language and it causes the chat to fail. I am curious as to why and how this happens. Additionally, why does this only happen in the temporary chat and not in the general chat? These are questions that I would like to know more about and I am curious to hear if anyone else has encountered the same experience with Chat GPT."
  }
]